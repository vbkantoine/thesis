

\chapter{Introduction}




\section{Motivation and positioning of the thesis}

\subsection{Uncertainty quantification in seismic probabilistic risk assessment studies}

% This section should be consituted by a introductory paragraph on the PRA, its rise of popularity over the deterministic method.
% How it covers a wide range of studies but which become essential in the context of nuclear industry and parasismic
% Eventually, how it is considered in France, and what is the role of the CEA in that whole concept


%Then, another paragraph relate to what does the concept of probabilistic introduction brings to the safety assessment studies, it can cite the essantialism of uncertainty quantification methods and sensitivity analysis tool. Indeed, those concepts have gained in popularity especially in the domains of fiability and safety. They regroup essential tools to provide margins and study the uncertainty between the inputs and the outputs of a model or a system (and as we should have said earlier, the nuclear industry is full of uncertainties). Uncertainties because of the thing that we do not control (epistemic ones?) and uncertainties because they are still things that we donnot understand (aleatoric ones)


\begin{itemize}
    \item The concept of probabilistic safety assessment studies
    \item The criticality of nuclear industry, risen of methods, PBEE, fragility curves in this context
    \item The SPRA in France, and in the CEA %\cite{roger_seisme_2020}
    \item Tools of UQ, Sensitivity analysis
\end{itemize}


\subsection{The choice of the prior in Bayesian studies}


% Here we should explian the basical concept of Bayesian inference as a comparison with the frequentist inference.
% In real world study, it can be seen as a way to manage uncertainty in a modeling, either by providing addtional information than the one provided by the data (so that by lowering the uncertainty) or in opposition by adding a substential uncertainty to avoid to ''overfit'' the data (idea to check).
% This modification of the uncertainty is defined through the prior that can have a significant influence on the posterior distribution: the distribution after the knowledge of the data. 
% Therefore, that distribution has to be sought very carefully. And the diffuctulty of that choice represents one of the principal ''holes'' in the Bayesian workflow according to Gelman.
% Examples of impact that the information might have on the data

% Because the prior impact results, which take the form of a interval, maybe it is interesting to clarify what such interval means, in comparison with a frequentist confidence interval.



\begin{itemize}
    \item Bayesian studies
    \item Their role in real world studies
    \item The impact of the prior and the research of objectivity
\end{itemize}

\subsection{Motivating prior elicitation research for SPRA studies}

%% We'll see if that section should not be part of the previous one.


% As the prior is chosen and has a significant influence on the result, its choice can be motivated by different needs. 
% In the case of the nuclear context and especially SPRA studies we seek a mthod that provide results which are auditable. The method must remains explainable, and thus, a subjective choice of a prior has to be aboided.

% As a matter of fact, a ''good'' study in SPRA for nuclear industry is not a study within which the predicted margins are small, or suggest that the system is safe. A good study is a study whose results can be trusted.
% The paradigm is complicated, as the operater seeks to prove that its structure belongs with a high probability in the margins fixed by the authority, but the safety expert want to be sure that all the uncertainty has been covered.

% A good prior, then, does not provide the smallest possible intervals of interest. But It must be supported by a proof of objectivity. A statement that would rensure the authority that the choice does not introduce a subjectivity that arange the operator











\begin{itemize}
    \item Bayes as a practical tool for prediction in UQ in SPRA
    \item The critical role of objectivity, in contexts with little datasets
\end{itemize}



\section{Outline of the manuscript and contributions}

\begin{itemize}
    \item The questions that this manuscript seeks to answer 
    \item Some objective of the thesis 
    % What are the ways to define objectively a prior? Are they totally objective? Are the objective priors the most intersting priors to consider, or should we discriminate judiciously some class of priors? How to compute and implement the objective priors? 
    % What does they look like in the context of SPRA? How to use them in that context and what are the obstacles of the Bayesian approach in the context? 
    \item Contributions in two areas: theory and practice. They actually are subtly intricate: practical needs motivated theoretical developments and theoretical results provided practical solutions to enhance the state-of-the-art
    \item Exhaustive list of contributions
\end{itemize}






\chapter{Introduction en français}

\section{Motivation et positionnement de la thèse}

\subsection{Etudes probabilistes de sureté}
%

\subsubsection{Historique}

Les études probabilistes de sureté (EPS) désignent un ensemble de méthodes d'analyses techniques qui permettent de quantifier un risque encourut par une installation lorsqu'elle est sujette à un évenement. L'évenement peut être d'origine naturelle comme artificielle, il peut être de provenance interne comme externe ; il peut s'agir d'un séisme, d'une inondation, d'une combinaison de défaillances internes, entre autres.  % (qui peut être d'origine naturelle comme artificielle, qui peut être interne comme externe). 

Faisant suite au premières recommandations de F. R. Farmer (expert sûreté à la UK atomic authority) dans les années 1960 pour la fiabilité des installations nucléaires,
ces méthodes ont pour caractéristique commune l'introduction de la notion d'incertitude dans la qualification de l'évenement, ses phénomènes et ses caractéristiques (on parle alors d'aléa).
%
%Elles ont été introduites par F. R. Farmer dans les années 1960 [], qui plébiscitait cette idée d'étudier la fiabilité des installations nucléaires en prenant en compte l'aspect probabiliste et incertain des évenements auqelles elle est sujette.
%
Leur cadre et leur concept ont été rapidement adoptés et développés aux Etats-Unis (cf. le raport de la Nuclear Regulatory Commission, \cite{nrc_pra_1983}). En particulier, de nombreuses études sont venues dès 1968 (\cite{cornell_engineering_1968}) 
y inscrire les analyses de fiabilité parasismique, définissant alors le cadre des études simsiques probabilistes de sureté (SPRA en anglais). %, en particulier en ce qui concerne le cadre de la fiabilité parasismque.



Le séisme représente en effet un facteur de risque remarquable des études de sureté.
Premièrement, bien que communément caractérisé par sa magnitude et sa distance à la source, son signal est bien plus riche qu'une fonction bivariée et deux séismes de même magnitue et de même source peuvent avoir des caractéristiques (et des conséquences) significativement différentes.
Deuxèmement, puisqu'il touche à la fois tous les élements aussi bien externes qu'internes à l'installation, il est la potentielle source de conséquences lourdes sur les équipements et structures.
%voir de réactions en chaîne 
%qui atteindraient le ``noyau dur'' de l'installation (défini comme un ensemble d'élements critiques à l'installation). %, impliquant ainsi un coût élevé.
Le coût potentiel des conséquences d'un aléa sismique peut alors être élevé et critique dans le contexte nucléaire, ce qui fait en fait un évenement d'intérêt majeur et décisif même dans les zones géographiques où il est rare.


%La prise en compte du séisme comme un aléa s'est confronté à la

Aujourd'hui, la prise en compte de l'aléa sismique sous le cadre des études probabilistes de sûreté est une recommandation internationale dans le contexte de l'industrie nucléaire. Leur cadre d'application dans l'industrie nucléaire française est précisé par l'autorité de sureté nucléaire et de radioprotection (ASNR)\footnote{Anciennement ASN: l'autorité de sûreté nuclaire (ASN), a été unifiée avec l'institut de radiprotection et de sûreté nucléarie (IRSN) le 1\textsuperscript{er} janvier 2025.} dans la règle fondamentale de sûreté (\cite{asn_regle_2002}).









\subsubsection{L'évolution des EPS en France, au CEA, et dans cette thèse}



%Le rôle des études de sûreté d'une manière générale est de produire une quantification du risque d'une installation, d'une marge de fiabilité, et de démontrer son repsect d'un seuil défini par une autorité de régulation.
% Deux visions s'oppose relativement au sujet de l'avolution des méthodes et des conaissances relatives aux études de sûreté.
%Lors de la mise en place de de seuil et de la règle, plusieur point de vue, motivés par plusieurs intérêts peuvent alors se heurter. Il y a d'une part les exploitants, pour qui la notion de robustesse perdure d'elle même, les marges prises sont là pour tenir compte du manque de conaissances, et il y a ceux pour lesquels la robustess est sans cesse remise en question par les avancées des méthodes et des connaissances (\cite{roger_seisme_2020}).



%L'évolutions des méthodes et des conaissances relatives à la sûreté de 
L'évolution des méthodes et des conaissances relatives à la sûerté des installations nucléaires se fait de manière parallèle à l'évolution de la règle et de la norme imposées à celles-ci. 
Sur le sujet de l'aléa sismique, le rapport à l'évolution de la règle et de la méthode (et donc des EPS) oppose différents points de vue, princpalement entre le principal exploitant (EDF) et les experts de l'autorité de sûreté (anciennement l'IRSN, maintenant unifié avec l'autorité de sûreté nucléaire devenue l'ASNR). 
Pour le premier, la robustesse de l'installation n'est normalement pas remise en question par l'avancée des conaissances et des méthodes puisque l’incertitude sur celles-ci fait parti des marges prises en comptes à la construction de l'équipement et calculées pour. Le second pense au contraire que la robustesse est une question perpétuelle et que la marge n’est pas pas faite pour être mordue au fil des connaissances qui s’ajoutent (\cite{roger_seisme_2020}). 
% L'évolution des méthodes EPS reste un champ de voies ouverte, pour lequel les approches s'opposent fondamentalement.
%Il est complxe de définir une voie d'évolution des EPS, et il n'y a pas de vision qui fait l'unanimté
L'ASNR se place alors en arbitre dans ce dialogue, entre autre elle définit le ``séisme majoré de sécurité''\footnote{Aujourd'hui plutôt appelé ``séisme noyau dur''.} (il s'agit d'une majoration du spectre du ``séisme maximal historiquement vraisemblable''\footnote{Aujourd'hui plutôt appelé ``séisme de dimensionnement''.}) qui sert de marge de référence dans la démonstration de robustesse parasismique d'un équipement. % la marge de fiablité et séisme maximal à prendre en compte. %En ce qui concerne l'aléa sismique, le dialogue 
%
%Un tel dialogue n'est pas propre à la France, et 


Cet arbitrage est donc à la fois sensible et critique.
L'incident survenu en 2011 à la centrale nucléaire de Fukushima-Daiichi le démontre. L'aléa sismique (qui est la cause du tsunami) de référence
%fixé par le concensus des experts japonais
a été sous-évalué par le consensus des experts japonais, et c'est cette sous-évaluation qui est au final la cause de l'incident (cf. le rapport de l'agence internationale de l'énergie atomique, \cite{iaea_fukushima_2015}).
%En 2011, a eu lieu un incident 
Suite à cet évenement, l'ASNR a pris la décision de majorer d'un facteur 1.5 le séisme majoré de sécurité en France, imposant une démonstration amplifiée de la robustesse des différentes installations par leur exploitant.



%En 2011, à la suite de l'incident à la centrale de Fukushima suite au séisme survenu au large des côtes japonaises, ce dialogue s'est conclut par une imposition de la part de l'autorité de sûreté d'une majoration d'un facteur de 1.5 de ce séisme majoré de sécurité en France. % de la marge de fiabilité sismique des équipements nucléaires en France. 

%Devant le coût induit par 

%Cette différence démontre de la complexité du sujet






% Le séisme ne fait pas exception
% la place des EPS simsique dans cette différence de vision est très marquée puisque l'aléa du séisme reste une quesiton ouverte en soi, la moindre différence de choix de séisme maximim possible peut donner lieu à une ampleur de changement d'un coût très élevé pour l'exploitant



%Les études probabilistes de sureté ont été adoptées progressivement en France 


%En France, l'industrie nucléaire est réduite à un nombre restreint d'acteurs, exploitants, experts indépendants, et autorité.

% Ambivalence de la robustesse, deux notions 
% deux visions
% Le CEA, à la fois exploitatn et expert

La place du CEA est ambivalente dans l'échiquier nucléaire français.
D'une part, il est exploitant d'installations nucléaires de recherche 
et joue alors son propre rôle quant à la démonstration de robustesse de ses équipements.
Aussi,
%, et ausi %titulaire d'une mission d'expertise de sûreté.
il participe à l'expertise conjointe des expertises de sûreté relatives au parc nucléaire civil exploité par EDF. 
%Le CEA joue son propre rôle dans l'évolution de la démonstration de la robustesse de ses propres equipements et installations. 
La recherche et l'expertise sur les causes de l'aléa sismique % études simqiques de sûreté au CEA 
se fait au CEA au laboratoire EMSI, qui dispose d'une plateforme experimentale (Tamaris) qui permet de procéder à divers tests mécaniques sur des équipements sous séismes. 
%Le laboratoire d'etudes mécaniques et sismiques (EMSI) s'inscrit pleinenemnt dans cette ambivalence. Propriétaire de la plateforme d'expérimentation sismique Tamaris, le labortoire se place souvent au centre des études de fiabilité sismique et du dialogue entre EDF et les experts de l'ASNR.
L'approfondisemnt des méthodes des études sismiques probabilistes de sûreté est un enjeux du CEA qui s'inscrit dans ce laboratoire. Le CEA étant responsable devant les autorités de sûrété de ses installations, il cherche %alors 
à développer des méthodes toujours rigoureuses devant leur vieillissement. %de ces installations et de leurs équipements.



% Le rôle du CEA est ambivalent dans cete equation, à la fois exploitant instut de recherche.
% Le CEA joue son propre rôle dans l'évolutions de la démonstration de robstesse de ses propres installations. Aussi il participe à l'expertise conjointe de méthodes d'EPS avec EDF.


% Le CEA est lui même exploitant, d'unstallations de recherche.
% Il a 

% L'evolutio des EPS y est toujours un enjeux pour être capable de produire une étude plus efficace, et des résultats plus convaincants aurpès des autorités.
% Le CEA est responsable auprès des autorités concernant ses installations et cherche à développer des méthodes toujours rigoureuses devant le vieillissemnt des installations

% Sur le sujet de l'aléa sismique, la démonstration de robustesse reste complexe vis à vis de la complexité de l'aléa lui-même, et de sa fréquence.
% Les EPS sont en constantes ré-évaluation à la demande des autorités de sûreté sur cette aspect, principalement depuis l'accident sur la centrale de Fukushima en 2011. %%% Peut etre mettre ca plus haut

% Au CEA, la recherche sur les études sismiques probabilistes de sûerté est portée par le laboratoire EMSI, qui dispose d'une plateforme experimentale (Tamaris) qui permet de procéeder à divers tests mécaniques sur des équipements sous séismes. 

Cette thèse s'inscrit dans la démarche d'évolution du SPRA et de démonstration de sûreté. Financée par le CEA dans le cadre d'un contrat de formation par la recherche, elle a pour objectif d'étoffer l'état de l'art en terme d'estimation de fragilité sismique des equipements et des installations.
L'objectif est de développer des méthodes qui doivent le mieux possible être (i) efficaces devant la complexité des objets étudiés et de la définition de l'aléa, (ii) être robustes dans le temps et devant une potentielle réévaluation des critères de sûreté, et (iii) être transparentes et auditable par les autorités de sûreté.

%lieu à des méthodes qui savent être efficace devant la complexité technique représentée efficaces devant la c

%en cherchant un cadre plus robuste, plus efficace.
%%%% Principalement en appuyant l'étude bayésienne de celles-ci.




\subsection{La quantification d'incertitudes dans les études probabilistes de sûreté} % la quantification d'incertitutde dans les EPS


\subsubsection{Principe et étapes de la quantification d'incertitudes}

%Le cadre des EPS regroupe un certains nmb
%Différents outils mathématiques sont employés dans le cadre des EPS et composent

Les études probabilistes de sûreté %et plus généralement les études probabilistes de fiabilité, de robustesse, et de  %les procédés de génie probabiliste d'études de fiabilité, de robustesse, et de 
s'appuient sur la 
prise en compte et la quantification des incertitudes qui apparaissent dans le procédé d'évaluation d'un risque au sein d'un système physique.
%lors de l'évaluation d'une quantité décrivant un risque. %d'un 
%
La quantification d'incertitudes représente en fait une démarche qui consiste à modéliser et analyser méthodiquement l'incertitude, sa source, et sa propagation au sein de la modélisation du système physique étudié. %étudié. %système phyisique?
Elle se construit alors à l'intéraction de la physique, de l'ingéniérie et des mathématiques appliquées, et s'érige même comme une branche des statistiques à part entière.

Elle concentre son étude sur 3 éléments qui décrivent l'intéraction entre le système physique et l'aléa : des paramètres d'entrées $\mbf X$, une réponse physique du système $\mbf Y$, et une modélisation de celle-ci selon : $\mbf Y=\cM(\mbf X)$. % ($\cM$ peut-être définit comme la ).
Cette fonction $\cM$ est liée à la modélisation, souvent complexe, du système et de ses proporiétés physiques (il peut s'agir de la résolution d'équations physiques, de simulation numériques, ou même du résultat d'une expérience mécanique).
%L'objet de la quantification d'incertitude est alors d'évaluer une quantité d'intérêt qui dépend de $\mbf Y$ : comme une variance, une probabilité de défaillance, etc ; plus généralement une quantité de la forme $\EE\,\phi(\mbf Y)$.
%
La démarche générale de la quantification d'incertitudes est souvent décrite au travers de différentes étapes clées (voir par ex. \cite{sudret_uncertainty_2007, iooss_contributions_2009}), à savoir : %au sein desquelles on distingue particulièrement l'étape d'identification des sources d'incertitudes.
%qu'on peutt principalement résumé 
%
\begin{enumerate}
    \item L'identification des sources incertitudes : 
        il est usuel de classifier les incertitudes selon deux grandes catégories. D'une part les incertitudes irréductibles, qui proviennent du hasard ``naturel'' embarqué dans l'aléa et le système physique. D'autre part, les incertitudes épistemiques, qui existent par manque d'information, et qui sont alors considérées comme réductibles en opposition aux précédentes (\cite{hullermeier_aleatoric_2019}). Il s'en suit une modélisation probabiliste de l'entrée $\mbf X$. 
    \item La propagation des incertitudes : cette étape correspondant à l'approximation de la distribution du modèle $\cM(\mbf X)$, et de l'évaluation d'une quantité d'intérêt qui dépend de celle-ci : comme une variance, une probabilité de défaillance, etc ; plus généralement une quantité de la forme $\EE\,\phi(\mbf Y)$.
%     et de celle de la quantité d'intérêt $\phi(\mbf Y)$. %De nombreuses méthodes mathématiques viennent en appui à cette étape, et sont évoquées plus bas.
    \item L'analyse de sensibilité : son rôle est de comprendre avec plus de recul la manière dont l'incertitude se propage dans un système, en identifiant l'impact d'un ou de plusieurs des paramètres d'entrées sur la sortie $\mbf Y$ (\cite{iooss_review_2015}).
    %L'analyse de sensibilité se construit depuis les premier travaux de Sobol' (\cite{sobol_sensitivity_1993})
    %, lorsque les entrées sont multivariées ($X=(X_1,\dots,X_p)$), quelles sont celles  
\end{enumerate}


\subsubsection{Des outils mathématiques en quantification d'incertitutdes}

%L'étape 1 sus-mentionnée s'ajoute à la modélisation du  modélisation
%Les étapes 2 et 3 ci-dessus mentionnées 
De nombreux outils mathématiques viennent appuyer les etapes sus-mentionnées. Bien que ce manuscrit n'a pas pour but d'en décrire un pannel exhaustif, ci-dessous sont listés quelques exemples.
%en voici quelques uns qui sont omniprésents dans le domaine

\begin{itemize}
    \item La métamodélisation %, %le plus souvent par processus gaussien (appellé aussi krigeage), ou par polynomes du chaos, 
    permet de contrevenir la complexité d'évaluation du modèle $\cM$, en construisant un métamodèle $\tilde\cM$, entraîné à partir d'une base de données d'observations $(\mbf X_1,\mbf Y_1),\dots,(\mbf X_k,\mbf Y_k)$. Le méta-modèle se veut par principe plus simple à évaluer que le modèle réel. 
    Rien ne limite sa définiton explicitement qui peut aller d'un simple modèle paramétrique à la sortie d'un réseau de neurones ; parmi les plus commun on distringue la méta-modélisation par processus gaussien (ou krigeage) et par polynômes du chaos.
    \item Les indices globaux de sensibilité s'inscrive pleinement dans l'étape d'analyse de sensibilité du système. Ils consistuent depuis les premiers travaux de Sobol' (\cite{sobol_sensitivity_1993}) des outils essentiels pour mesurer statistiquement comment $\mbf Y$ est impactée par un ou plusieurs des $X_i$ (où $(X_1,\dots,X_p)=\mbf X$). Dans ce cadre, l'impact, noté $S_i$, de l'entrée $X_i$ sur $\mbf Y$ s'exprime comme une divergence moyenne entre la distribution $\PP_{\mbf Y}$ de $\mbf Y$ et sa distribution conditionalement à $\mbf X$, $\PP_{\mbf Y|\mbf X}$ (\cite{da_veiga_global_2015}) : 
        \begin{equation}
            S_i = \EE_{X_i}[D(\PP_{\mbf Y}||\PP_{\mbf Y|X_i})],
        \end{equation}
    où $D$ est une mesure de dissimilarité entre deux mesures de probabilité.
    Le choix de $D$ est alors décisif dans l'étude de $S_i$. %, qui revient à une étude de divergence entre deux mesures de probabilité. 
    On peut noter, par example, que choisir $D$ définie par $D(P||Q) = \|\EE_{X\sim P}X-\EE_{X\sim Q}X\|^2$ revient à étudier un indice de Sobol' du premier ordre.
    Le choix de $D$ peut être motivé par divers intérêts, par exemple, celui de discriminer  l'indépendance entre $X_i$ et $\mbf Y$. Le point suivant détail un large pannel de choix possible pour $D$.

    \item La théorie de l'information est au centre de la comparaison de mesures de probabilité et donc de la quantification d'incertitudes. Par essence, les manières de comparer la façon qu'ont deux mesures de probabilité de  distribuer l'information dans un espace sont vastes. 
    Dans la définition de $S_i$ plus haut, le choix de $D$ (et donc le choix de définition de $S_i$) relève de la théorie de l'information.
    
    
    % , puisque deux distributions difères par la manière dont elles 
    Un choix alors commun, perçu comme une extention de l'entropy de Shanon, est celui des $f$-divergences de \citet{csiszar_information-type_1967}. On peut noter qu'elles sont employées dans des domaines variés, au delà de l'analyse de sensibilité, tel que l'inférence variationnelle (\cite{minka_divergence_2005,bach_sum--squares_2023}), le design de méta-modèles (\cite{nguyen_surrogate_2009}), l'apprentissage PAC-bayésien (\cite{picard-weibel_change_2022}).
    Lorsque $f$ est convexe que $f(1)=0$, elle sont définies selon :
        \begin{equation}\label{eq:intro-fdiv}
            D_f(P||Q) = \int_\cX f\left( \frac{p(x)}{q(x)} \right) q(x)d\mu(x),
        \end{equation}
    en notant $p$, $q$ des densités associées à $P$ et à $Q$ par raport à une mesure commune $\mu$ sur leur ensemble de définition $\cX$.     
    Les $f$-divergences réduisent le choix de $D$ à celui de $f$, les plus communément employées étant les $\alpha$-divergences (\cite{zhu_information_1995}) définies en fixant $f=f_\alpha$ où
        \begin{equation}
            f_\alpha(x) = \left\lbrace\begin{array}{l l} \frac{x^\alpha-\alpha t-(1-\alpha)}{\alpha(\alpha-1)} & \text{si\ }\alpha\not\in\{0,1\},\\ 
                t\log t-t+1  & \text{si\ }\alpha=1, \\
                -\log t +t -1 & \text{si\ }\alpha=0.
            \end{array}\right. 
        \end{equation}    
    Il est remarquable que la peut-être plus connue des mesure de dissimilarité (la divergence de Kullback-Leibler) peut être vu comme un cas particulier des $\alpha$-divergences. En effet, rappelons que la divergence de Kullback-Leibler, notée $\text{KL}$, est définie par 
        \begin{equation}
            \text{KL}(Q||P) = \int_\cX\log\left(\frac{q(x)}{p(x)}\right)q(x)d\mu(x)
        \end{equation}
        (en adoptant les même notations qu'en équation \eqref{eq:intro-fdiv}), si bien que 
        \begin{equation}
            D_{f_\alpha}(P||Q) = \text{KL}(P||Q)\ \text{si\ }\alpha=1,\quad D_{f_\alpha}(P||Q) = \text{KL}(Q||P)\ \text{si\ }\alpha=0.
        \end{equation}

    Bien sûr, il existe aussi de nombreuses mesures de dissimilarité qui ne sont pas des $f$-divergences, à ce titre, les indices de Sobol' du premier ordre évoqués plus haut font offices d'exemple. Un autre exemple notable présenté ici est celui des normes maximales de dicrépences (MMD). Soient $P$, $Q$ deux mesures de probabilités définies sur un ensemble $\cX$, et soit $\cH$ un espace de Hilbert à noyau reproduisant (RKHS) sur $\cX$,  dont le noyau reproduisant est noté $k:\cX\times\cX\to\CC$. %On peut définir les imbrications moyennes  \cite{gretton_kernel_2012}
    On définit la MMD (\cite{gretton_kernel_2012}) par
        \begin{equation}
            \text{MMD}(\cH;\,P||Q) = \sup_{\substack{f\in\cH\\ \|f\|_\cH\leq1}} |\EE_{X\sim P}f(x) - \EE_{X\sim Q}f(X)|,
        \end{equation}
    ou, dans une forme plus simple :
        \begin{equation}
            \text{MMD}^2(\cH;\,P||Q) = \EE_{X,X'\sim P\otimes P}[k(X,X')] + \EE_{Y,Y'\sim Q\otimes Q}[k(Y,Y')] - 2\EE_{X,Y\sim P\otimes Q}[k(X,Y)].
        \end{equation}
        %
    \item L'analyse bayésienne est outil incontournable de la quantification d'incertitude, puisqu'elle a pour principe fondamental d'introduire une forme d'incertitude dans un ou plusieurs des paramètres du modèle étudié. 
    L'incertitude incorporée par le cadre bayésien peut prendre diverse forme, et se traduit au travers de la définiton de l'\emph{a priori}. 
    La propagation d'incertitudes au travers de ce canevas se quantifie en calculant l'\emph{a posteriori}.
    La définition de l'\emph{a priori} est critique, elle prend place dans l'étape d'identification des incertitudes, et se révèle impactante sur l'\emph{a posteriori}, et donc, dans un contexte d'étude probabiliste de sûreté, sur le risque.
    
    Puisque ce dernier outil, qui est en fait un paradigme à lui tout seul, est au centre de cette thèse, la section qui suit lui est pleinement dévoué.
\end{itemize}
















\subsection{Le choix du prior dans les études Bayésienne}

L'inférence Bayésienne s'inscrit dans le domaine de l'inféérence statistique.
% ici paradigmes freq bayes et intervalles cred conf





\subsection{Ce qui motive la recherche sur l'elicitation de prior pour les Etudes probabilistes de sûreté}


L'étude Bayésienne a gagné en popularité dans le cadre des études probabilistes dans de nombreux domaines, y comprit en sûreté. Elle fait partie des outils proposés dans le référentiel de sûreté de l'ASNR. 
Elles sont plébiscité par leur capacité dites 'régularisatrices' et leur ouverture à l'introduction de jugement d'experts a priori.

En réalité, c'est surtout ce dernier point qui fait débat. Dans denombreux cas, les jugement sd'experts ne sont pas unanimes, et si on trouve autant de jugements qu'il y a d'experts, alors on se questionnera sur leur aspect objectif.









\section{Esquisse du manuscrit et contributions}








