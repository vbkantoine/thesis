

\begin{abstract}
    The reference prior theory is a prominent tool when 
    one researches for a prior embedded with an objective construction. It is designed to ensure the information in the posterior comes promptly from the observations rather than from the prior itself.
    In this chapter, the theory is reviewed, and its formalism is rigorously introduced.
    We recall the main results about the reference priors that are proven in the literature.
    Additionally, because we find the usual Bayesian framework limiting for the study of reference priors, we propose a novel Bayesian formalism that provides a rigorous incorporation of improper priors.
    The latter fixes the notations we use in rest of the manuscript.
    % We provide a comprehensive state-of-the-art. 
    % this chapter is also the occasion to introduce 
    % %We review the reference prior theory
\end{abstract}

\minitoc


\section{Introduction}\label{sec:intro-ref:intro}

Bayesian analysis offers a coherent framework for integrating prior information and updating beliefs with data. The incorporation of prior knowledge can be motivated by various needs, such as enhancing interpretability, introducing uncertainty, or embedding existing knowledge into the analysis. Actually, the selection of this a priori centralizes a lot of attention in Bayesian inference and can be a critical aspect of the method. As a matter of fact, it can significantly influence the posterior distribution and, consequently, the conclusions drawn from the analysis. Thus, an inconsistent or insufficiently justified choice of prior can jeopardize the validity of the entire study, a concern now well identified in the modern Bayesian workflow described in \cite{gelman_bayesian_2020}, and echoed across applied domains.\\

A plethora of approaches exist in the literature to address the prior elicitation problem. They offer different strategies for different purposes. 
Comprehensive reviews of such methods are proposed in \cite{mikkola_prior_2023}
and in \cite{consonni_prior_2018}, for instance.
Among the different challenges that are commonly addressed by these methods, we can emphasize (i) the incorporation of information, (ii) the ease of posterior inference, or (iii) the research for interpretability.\\
The former refers to the so-called ``informative priors'', because their distributions tend to concentrate the information. %in some area of the doamin.  significantly inform the posterior distribution with strong distribution tails.
They aim to incorporate genuine prior knowledge about parameters, often arising from past data, domain expertise, physical constraints, or theoretical understanding. %They are particularly common in fields like clinical trials, risk analysis, ecology, and engineering, where historical information is abundant or regulatory frameworks encourage explicit prior modeling.
The difficulty lies in doing so transparently, reproducibly, and justifiably, especially when seeking to avoid subjective bias.

Regarding the second, we can recall conjugate priors (see e.g. \cite{robert_bayesian_2007}) that are designed to provide a simple formulation of the posterior. Penalized Complexity (PC) priors \citep{simpson_penalising_2017} and sparse priors \citep{castillo_bayesian_2015} are made to be suitable for high-dimensional problems; and g-priors \citep{liang_mixtures_2008} are used for variable selection in normal regression models.

Thirdly, in order to provide a better interpretability of the information transmission process, several studies favor a hierarchical design, in which the prior is the result of a latent prior modeling. This approach allows the use of historical data, as in power priors \citep{chen_relationship_2006} and Information Matrix priors \citep{gupta_information_2009}; or even imaginary data \citep{perez_expectedposterior_2002,spitzner_neutral-data_2011}. Posterior priors, developed for sensitivity analysis studies \citep{bousquet_discussion_2024}, are also part of this approach. %with imaginary data. 
These hierarchical constructions are appreciated for their ability to tackle the improper aspect (they integrate to infinity) of most low-informative priors
(priors whose distribution is more diffuse through the parameter's space, in opposition with informative priors).\\



Many of these  methods involve subjective choices that remain open to criticism. Often, they are based on the selection of a class and hyperparameters that remain to be tuned.

The research for so-called ``objective priors'' arose in contexts where the incorporation of uncritical beliefs is impossible (either by lack of available beliefs, or by lack of confidence within the existing ones).
In those contexts, there is consensus that an appropriate prior belongs within the non-informative ones. In this respect, \citet{lindley_measure_1956} already suggested the maximization of the Shannon's entropy, as a way to choose a prior that would provide the least possible information.
The same philosophy has been the source of several rules, which lead to different definitions of objective priors. \textcolor{orange}{A review of the  existing rules and their application on simple examples is proposed by \cite{kass_selection_1996} and by \citet{berger_overall_2015}. We also mention the work of  \citet{datta_invariance_1996}, in which the question of invariance by re-parametrization of several of these rules is thoroughly focused.}
%propose a review of these rules and provide applications of them on simple examples.}



% \citep{kass_selection_1996, datta_invariance_1996,berger_objective_2008}, which lead to different definitions for objective priors.

In this chapter and in this thesis, we focus on the reference prior theory, first introduced by \citet{bernardo_expected_1979}. 
Based on an ``expected utility maximization'',
the idea was to shift from minimizing the information in the prior to maximizing the knowledge brought by the observations over prior distribution itself.
This notion of expected information, is defined with the support of the mutual information which allows for a formal construction of priors that are designed to let the data dominate the inference.

The reference prior theory has been extensively studied since its deepen formalization by \citet{berger_formal_2009}. 
\textcolor{orange}{The derivation of reference priors and the study of their properties in different contexts has been extensively focused. We can mention the works in \cite{berger_objective_2008} for bivariate normal models, 
 \cite{berger_ordered_1992} for multinomial models,  and \cite{gu_parallel_2016} for Gaussian process models, among others. 
More recent studies on the theory are scarce, yet we can
we cite the work of \citet{bodnar_analytical_2014}, who suggest the derivation of reference priors using sequential maximization of Shannon's entropy, and the work of \citet{gao_deep_2022} 
who consider reference priors to the training of deep neural networks. 
%A few developments based on extending the definitions of the reference priors have been recently proposed
%and other works suggesting a generalization of the reference prior defintion \cite{hashimoto_reference_2021,le_formal_2014,liu_divergence_2014} 
%\citet{clarke_jeffreys_1994}
%who provide a general formulation 
%The reference priors themselves have deri
%We can mention several works t
We mention that \citet{mure_objective_2018} provides a comprehensive review of the theory, and introduces original results about the uniqueness of the reference priors.}

This chapter proposes a novel brief review of the reference prior theory. We fix the framework and define important main tools for the theory in \cref{sec:intro-ref:classical-framework}. %, such as the mutual information. 
The reference priors are formally defined in \cref{sec:intro-ref:mutalinfo-ref-priors}, and we review its different properties that are proven in the literature.
Eventually, this chapter is the occasion to discuss the limitations of the standard framework of the reference prior, and to suggest a solution based on an original formalization of the Bayesian inference tools.  The latter in done in \cref{sec:intro-refs:limits} and fixes the mathematical elements and notations that we will manipulate through the whole manuscript. A short conclusion terminates the chapter in \cref{sec:intro-ref:conclusion}.






%The different approaches are generally categorized within two classes: informative and non-informative priors.


%Despite their variety, many of them involve subjective choices that remain open to criticism. Indeed, prior elicitation methods generally con- sist of choosing a way to inform a prior. They are often based on the selection of a class and hyperparameters that remain to be tuned.

% Informative priors aim to incorporate genuine prior knowledge about parameters, often arising from past data, domain expertise, physical constraints, or theoretical understanding. They are particularly common in fields like clinical trials, risk analysis, ecology, and engineering, where historical information is abundant or regulatory frameworks encourage explicit prior modeling.


\section{The standard Bayesian framework}\label{sec:intro-ref:classical-framework}

\subsection{A minimal framework}\label{sec:intro-ref:minimalclassicframework}

The Bayesian framework is commonly defined from the given of statistical model.
Let $(\Omega,\sP,\PP)$ be a probability space, and define $Y$, a random variable defined on $\Omega$ and taking values on a measurable space $(\cY,\sY)$.
A statistical model is characterized by a collection of parameterized probability measures $(\PP_{Y|\theta})_{\theta\in\Theta}$ on $(\cY,\sY)$.
The Bayesian viewpoint considers a random variable $T:\Omega\to\Theta$ taking values in a measurable space $(\Theta,\sT)$, following a \textbf{prior distribution} denoted $\varPi$, and which is such that the conditional distribution of $Y$ to $T=\theta$ is $\PP_{Y|\theta}$. In other terms:
    \begin{equation}
        \forall A\in\sY,\,B\in\sT,\, \PP(Y\in A,\, T\in B) = \int_B \PP_{Y|\theta}(A) d\varPi(\theta).
    \end{equation}
%Note that the above quantities are well-defined and are almost surely unique, at least while $(\cY,\sY)$, $(\Theta,\sT)$ and $(\PP_{Y|\theta})_\theta$ align with the statements of the disintegration theorem (see e.g. \cite{chang_conditioning_1997}).

In practice, $k$ realizations $\mbf y= (y_1,\dots,y_k)$ of the r.v. $Y$ are observed, they are supposed to be identically distributed and independent conditionally to $T$, meaning they are the realization of a random vector $\mbf Y_k = (Y_1,\dots,Y_k)$, whose distribution $\PP_{\mbf Y_k}$ is defined on $(\cY^k,\,\sY^{\otimes k})$ by:
\begin{equation}
    \forall A\in\sY^{\otimes k},\, \PP_{\mbf Y_k}(A) = \int_\Theta \PP_{\mbf Y_k|\theta}(A)d\varPi(\theta),
\end{equation}
where $\PP_{\mbf Y_k|\theta}:=\PP_{Y|\theta}^{\otimes k}$. The distribution $\PP_{\mbf Y_k}$ is called the \textbf{marginal distribution}.

Given the observations $\mbf y$, the \textbf{posterior distribution}, denoted $\PP_{T|\mbf y}$, is then defined as the $\PP_{\mbf Y_k}$-a.s. unique one verifying
    \begin{equation}
        \forall A\in\sY^{\otimes k},\,\forall B\in\sT,\, \int_{A}\PP_{T|\mbf y}(B)d\PP_{\mbf Y_k}(\mbf y) = \int_B \PP_{\mbf Y_k|\theta}(A)d\varPi(\theta).
    \end{equation}
We can notice that the posterior distribution is always absolutely continuous w.r.t. the prior distribution.
Its existence and $\PP_{\mbf Y_k}$-a.s. uniqueness are ensured by the disintegration theorem (see e.g. \cite{chang_conditioning_1997}), while $(\cY,\sY)$ and $(\Theta,\sT)$ are Borel spaces.



\subsection{Likelihood and densities}\label{sec:intro-ref:likelihoods}


It is common to assume that the statistical model admits a likelihood: there exists a collection of density functions $(\ell(\cdot|\theta))_{\theta\in\Theta}$  w.r.t. a common  $\sigma$-finite measure $\mu$ on $\cY$ such that for $\varPi$-a.e. $\theta\in\Theta$:
    \begin{equation}
        \forall A\in\sY,\,\PP_{Y|\theta}(A) = \int_A\ell(y|\theta)d\mu(y).
    \end{equation}

\color{orange}
\begin{ex}[Multinomial model]\label{ex:intro-ref:multinomialmodel}
    The multinomial model is an example of statistical model where $\theta\in(0,1)^d$, $\cY=\{y\in\{0,\dots,n\}^d,\,\sum_{i=1}^dy^i=n\} $ for a fixed $n\in\NN^\ast$, and with
        $%\begin{equation}
            \ell(y|\theta) = \frac{n!}{y^1!\dots y^d!}\prod_{i=1}^d\theta_i^{y^i}.
        $%\end{equation}
\end{ex}
\color{black}

This way, $\PP_{\mbf Y_k}$ admits a \textbf{marginal density} $p_{\mbf Y_k}$ w.r.t. $\mu^{\otimes k}$ defined as
    \begin{equation}
        \forall\mbf y\in\cY^k,\, p_{\mbf Y_k}(\mbf y) = \int_\Theta\prod_{i=1}^k\ell(y_i|\theta) d\varPi(\theta) = \int_\Theta \ell_k(\mbf y|\theta)d\varPi(\theta),
    \end{equation}
where $\ell_k(\mbf y|\theta)$ denotes $\prod_{i=1}^k\ell(y_i|\theta)$ and is called the likelihood.

Also, we can suppose that $\varPi$ admits a \textbf{prior density} $\pi$ w.r.t. a $\sigma$-finite measure $\nu$ on $\Theta$ (usually, it is simply assumed that $\Theta\subset\RR^d$ and $\nu$ is the Lebesgue measure). 
%We acknowledge that it is usual to make the confusion between the prior distribution and its density, while there is no ambiguity doing so. 
%Thus, we denote by $\pi$ the density of $\i$ w.r.t. $\nu$. 
The posterior distribution admits a \textbf{posterior density} $p(\cdot|\mbf y)$ w.r.t. $\nu$ defined by
    \begin{equation}
        \forall\theta\in\Theta,\, p(\theta|\mbf y) = \frac{\ell_k(\mbf y|\theta)\pi(\theta)}{p_{\mbf Y_k}(\mbf y)}\text{\ if\ } p_{\mbf Y_k}(\mbf y)\ne0,\ p(\theta|\mbf y)=1 \text{\ otherwise}.
    \end{equation}
%The same way, 
To conclude this section, we acknowledge the usual confusion between the notation for the random variable $T$ and notation for the values it takes $\theta$. Thus, we might refer as ``the distribution of $\theta$'' or the ``distribution conditionally to $\theta$'' to respectively mention the prior distribution or the conditional distributions to $T=\theta$.


\subsection{Improper priors}\label{sec:intro-ref:improperway-around}

It is common when dealing with non-informative priors in Bayesian analysis to define improper ones.
Improper priors are defined as distributions whose density $\pi$ integrates to infinity: $\int_\Theta\pi(\theta)d\nu(\theta)=\infty$. 

Of course, such a prior is not a probability distribution anymore, so that the framework elucidated in \cref{sec:intro-ref:minimalclassicframework,sec:intro-ref:likelihoods} does not stand. 
A way-around to be able to use improper priors is to focus on restrictions of the model, %In precise terms,  
as formalized in the definition below.
\begin{defi}
    As $\nu$ is a $\sigma$-finite measure, there exists an increasing sequence $(B_n)$ of elements in $\Theta$ such that 
$\Theta=\bigcup_{n\in\NN}B_n$ and $\nu(B_n) <\infty$.
An improper prior is said to be admissible if for any $n$, $\int_{B_n}\pi(\theta)d\nu(\theta)<\infty$, and if there exists $n_0$ such that $\int_{B_{n_0}}\pi(\theta)d\nu(\theta)>0$.
\end{defi}

In this case, on any $\tilde\Theta\in\sT$ such that there exists $N\in\NN$ verifying $\tilde\Theta\subset\bigcup_{n\geq n_0}^NB_n$ and such that $B_{n_0}\subset\tilde\Theta$, one can define the probability space $(\tilde\Theta,\tilde\sT,\tilde\varPi)$ with:
    \begin{equation}
        \tilde\sT = \{B\cap\tilde\Theta,\,B\in\sT\},\quad\forall \tilde B\in\tilde\sT,\,\tilde\varPi(\tilde B) = \frac{\int_{\tilde B}\pi(\theta)d\nu(\theta)}{\int_{\tilde\Theta}\pi(\theta)d\nu(\theta)}.
    \end{equation}
%Thus, it is possible to 
%    Thus, restricting the study to $\tilde\Theta$ let the modeling of previous sections to stand, 
That modeling is the restricted modeling driven by $\tilde\Theta$. 
The probability $\Tilde\varPi$ (respectively its density) is referred as the renormalized restriction of the prior $\varPi$ (resp. of the prior density $\pi$) to $\tilde\Theta$.

On any restricted model, the posterior and the marginal distributions exist. 
Given two restrictions driven by $\tilde\Theta_1$ and $\tilde\Theta_2$, the renormalized restricted prior densities $\tilde\pi_1$ and $\tilde\pi_2$ are equal on $\tilde\Theta_1\cap\tilde\Theta_2$ up to a constant:
    \begin{equation}
        \tilde\pi_1(\theta) = K\tilde\pi_2(\theta),\quad\text{so that}\quad \forall \mbf y\in\cY^k,\,\tilde p_{1,\mbf Y_k}(\mbf y) = K\tilde p_{2,\mbf Y_k}(\mbf y),
    \end{equation}
where $\tilde p_{1,\mbf Y_k}$ (reps. $\tilde p_{2,\mbf Y_k}$) denotes the marginal density in the restricted model driven by $\tilde\Theta_1$ (resp. $\tilde\Theta_2$). Thus, calling $\tilde p_1(\cdot | \mbf y)$ (resp. $\tilde p_2(\cdot | \mbf y)$) the posterior density given the observations $\mbf y$ and under the restricted model driven by $\tilde\Theta_1$ (resp. $\tilde\Theta_2$), we have
    \begin{equation}
        \forall\theta\in\tilde\Theta_1\cap\tilde\Theta_2,\, \tilde p_1(\theta | \mbf y) =\tilde p_2(\theta | \mbf y).
    \end{equation}
We conclude that the posterior density is always well-defined on $\Theta$ when the improper prior is admissible.
If the posterior density integrates to $\infty$, the posterior is said to be improper.



This construction is called a way-around because the framework remains incomplete: $T$ is not embedded with a probability distribution  anymore, and %appropriately defined anymore, nor 
the marginal distribution $\PP_{\mbf Y_k}$ in not appropriately defined. 
Later in this chapter, we will introduce a novel construction that authorizes the definition of improper priors in a more suitable way.
Before that, the current construction is enough to define the reference priors among the admissible priors: priors that are either proper, either admissible improper priors.


%We denote that density by $\pi$ as well, this slight abuse 

\section{Mutual information and reference priors}\label{sec:intro-ref:mutalinfo-ref-priors}

\subsection{Mutual information}\label{sec:intro-ref:mutualinfo}


The principle of the theory 
is to minimize the role of the prior within the posterior, in order to maximize the information gain from the data.
Thus, it amounts to maximize how ``away'' is the prior to the posterior.

When it resorts to compare information between probability distributions, we generally consider dissimilarity measures. The most common one is the Kullback-Leibler divergence, it is the one that is considered in the historical settings of the reference prior theory.
We recall below the expression of the Kullback-Leibler divergence between two distribution $P$ and $Q$ on $\cX$, which admit densities $p,q$ w.r.t. a common measure $\omega$:
    \begin{equation}
        \text{KL}(Q||P)=\int_\cX\log\left(\frac{q(x)}{p(x)}\right)q(x)d\omega(x).
    \end{equation}

Thus, the idea is to maximize the divergence $\text{KL}(\PP_{T|\mbf y}||\varPi)$. As the observed sample $\mbf y$ is unknown, the average value of the divergence is considered:
\begin{defi}[Mutual information]\label{def:intro-ref-MI}
    Given a Bayesian framework defined as in \cref{sec:intro-ref:classical-framework}, and given a number of observations $k$, the mutual information is defined as the quantity
    \begin{equation}
        \sI^k(\varPi) :=  \EE_{\mbf Y_k\sim \PP_{\mbf Y_k}}[\text{KL}(\PP_{T|\mbf Y_k}||\varPi)] =  \int_{\cY^k}\text{KL}(\PP_{T|\mbf y}||\varPi) p_{\mbf Y_k}(\mbf y)d\mbf y.
    \end{equation}
\end{defi}


%This quantity is called the mutual information.

We recall that the definition of the mutual information is not limited to the scope of Bayesian analysis. In information theory it is common to refer to the mutual information $\sI(X,Z)$ between a random variable $X$ and a random variable $Z$ (see e.g. \cite{mackay_information_2003}). The quantity in \cref{def:intro-ref-MI} aligns with their definition of the mutual information $\sI(T,\mbf Y_k)$ between the random parameter $T$ and the random data $\mbf Y_k$.

One can apply Fubini-Lebesgue's theorem to write the mutual information as
\begin{equation}
    \sI^k(\varPi) =  \EE_{\mbf Y_k\sim \PP_{\mbf Y_k}}[\text{KL}(\PP_{T|\mbf Y_k}||\varPi)] =  \EE_{T\sim\varPi}[\text{KL}(\PP_{\mbf Y_k|T}||\PP_{\mbf Y_k}) ]
\end{equation}
This second expression allows an interpretation of the mutual information from another viewpoint: it measures how the parameter $T$ impacts the distribution of $\mbf Y_k$.



%In the next section, a classe of 







\subsection{Reference priors}\label{sec:intro-ref:refpriors}

Reference priors are defined as the priors that are expected to maximize the role of the observed data within the posterior distribution.
Consequently, they are historically sought as maximizers of the mutual information.

However, formally maximizing the mutual information $\sI^k(\varPi)$ w.r.t. $\varPi$ presents three main issues: %, as its expression depends on the value of $k$.
\begin{enumerate}
    \item First, its expression depends on the value of the number of  observations $k$, and $k$ alters the form of its maximal argument. %The prior expression comes, by nature, prior to the observations, and should depend neither on $k$, neither on $\mbf y$. A 
    %In normal settings, there are no knowledge on the v
    This problem is tackled arguing that $\sI^k$ only measures a limited quantity of information brought by $T$ onto the distribution of the data $\mbf Y$. The Bayesian framework does considerate the data $(Y_1,\dots,Y_k)$ to be dependent, so that the distribution of $\mbf Y$ is enriched as $k$ rises.
    According to \citet{bernardo_bayesian_1994}, the limit $\sI^\infty$ (if it exists) measures the knowledge missing form the prior.
    Thus, the formal definition tends to maximize $\sI^k$ as $k$ diverges to $\infty$.
    \item The second issue comes with the fact that the quantity $\sI^k(\varPi)$ generally does not admit a finite limit as $k$ diverges to $\infty$.
    \item The third issue is that the mutual information is only correctly defined when the prior is proper, while many non-informative priors used in Bayesian analysis are improper.
\end{enumerate}

The commonly admitted definition for the reference prior, which takes into account the three issues aforementioned is the following, adapted from \cite{dey_reference_2005}.
\begin{defi}[Reference priors]\label{def:intro-ref:ref-priors}
    Let $\cP$ be a set of priors on $\Theta$, a prior $\varPi\in\cP$ %whose prior density is $\pi$ 
    is a reference prior over $\cP$ if there exists an appropriate increasing sequence $(\Theta_i)_{i\in\NN}$ such that $\bigcup_{i\in\NN}\Theta_i=\Theta$ with for any $i$: $0<\varPi(\Theta_i)<\infty  $ and
        \begin{equation}
            \lim_{k\rightarrow\infty} \sI^k_i(\varPi_i) - \sI^k_i(P_i) \geq 0 \text{\ for all\ } P\in\cP_i
        \end{equation}
    %with equality if and only if $\varPi_i=P$; 
    with $\varPi_i$ (resp. $P_i$) being the renormalized restriction of $\varPi$ (resp. $P$) on $\Theta_i$, $\sI_i^k$ denoting the mutual information on the restricted modeling driven by $\Theta_i$, and $\cP_i=\{P\in\cP,\,0<P(\Theta_i)<\infty\}$.
\end{defi}

%This definition differs slightly from the one in \cite{berger_formal_2009}
%This definition will serve as a reference. I
This first definition considers a set $\cP$ called a set of priors. 
We provide a thorough definition of such a class in \cref{sec:intro-refs:limits}.
Until then, it refers to a set of priors that are admissible as described in the \cref{sec:intro-ref:improperway-around}.
This definition also
requires the sequence $(\Theta_i)$ to be ``appropriate''.
Actually,  the original definition does not restrict the choices of that sequence, so that this word can be ignored in this section.
However, we will see in the sequel that some studies on reference priors require to impose some assumptions on $(\Theta_i)_i$. 
%we will that some assumptions are mode on them by 

\textcolor{orange}{
\begin{ex}
    If $\Theta$ is finite with $\sT=\sP(\Theta)$, and if $\PP_{Y|\theta}\ne\PP_{Y|\theta'}$ for any $\theta'\ne\theta$ then the reference prior $\varPi$ over the set of all priors is the uniform prior: $\varPi(B)=\varPi(B')$ for all $B,B'\subset\Theta$. A proof of this result is proposed in \cite{mure_objective_2018}.
\end{ex}
}


We must note that other definitions of reference priors exist.
In the geneses of the reference prior theory, \citet{bernardo_reference_1979} suggested maximizing the mutual information $\sI^k$, and derive the limit of the maximizers when $k\to\infty$ to define reference priors.
In \cite{berger_formal_2009}, the authors prove that in the real and one parameter case (i.e. $\Theta\subset\RR$), this strategy is consistent with \cref{def:intro-ref:ref-priors}. Their result is the following, sometimes taken as a definition.

\begin{thm}[Explicit for of the reference prior]\label{thm:intro-ref:explicitRP}
    Assume $\Theta\subset\RR$ with $\nu$ being the Lebesgue measure.\\
    Call $\cP_s$ the set of admissible priors that are positive and continuous, and that issue proper posterior distributions.\\
    Let $\varPi^\ast\in\cP_s$ we call $p^\ast(\cdot|\cdot)$ its posterior and define for any interior point $\theta_0\in\Theta$,
        \begin{equation}
            % \begin{aligned}
                f_k(\theta) = \exp\left(\int_{\cY^k}\ell_k(\mbf y|\theta)\log(p^\ast(\theta|\mbf y)) d\mu^{\otimes k}(\mbf y) \right) \quad\text{and}\quad  %\\
                f(\theta) = \lim_{k\rightarrow\infty}\frac{f_k(\theta)}{f_k(\theta_0)}.
            % \end{aligned}
        \end{equation}
    If: (i) $ \forall \theta,\eps>0,\,\int_{|\tilde\theta-\theta|<\eps} p^\ast(\tilde\theta|\mbf y)d\tilde\theta \conv[\PP_{\mbf Y|\theta}]{k\rightarrow\infty}1$,\\ (ii) each $f_k$ is continuous with $\forall k,\theta,\, \frac{f_k(\theta)}{f_k(\theta_0)} <h(\theta)$ such that $h$ is integrable on any compact set, and\\
    (iii) $f$ is the density of $F\in\cP_s$  and is such that for any increasing sequence of compact sets $(\Theta_i)_{i\in\NN}$ that covers $\Theta$, calling $g_i$ the restricted posterior density of $F$ on $\Theta_i$, we have $\forall \mbf y,\, \int_{\Theta_i}g_i(\theta|\mbf y)\log\frac{g_i(\theta|\mbf y)}{g(\theta|\mbf y)}d\theta \conv{i\rightarrow\infty}0$ where $g$ is $F$'s posterior,\\
    then $F$ is a reference prior over $\cP_s$.
 \end{thm}

 \textcolor{orange}{
The form of the functions $f_k$ in the above theorem comes from maximizing the mutual information when $k$ is fixed. Indeed, it is possible to research for a maximal argument of the mutual information
form a constrained optimization viewpoint. A Lagrange multipliers theorem should give
an implicit expression of the density whose prior maximizes the mutual information for a fixed $k$: it is
proportional to 
    $\exp\left(\int_{\cY_k}\ell_k(\mbf y|\theta)\log(p(\theta|\mbf y) ) d\mu^{\otimes k}(\mbf y)\right)$. 
This result is rigorously proven in the appendix for a particular case. 
It was intuited by \citet{bernardo_bayesian_1994}, with the idea that, asymptotically, the posterior becomes independent of the chosen prior. The theorem formalizes the intuition, as the posterior in $f_k$'s expression comes from any initial prior $\varPi^\ast\in\cP_s$.
}
%


\Cref{def:intro-ref:ref-priors} and \cref{thm:intro-ref:explicitRP}
both define reference priors as maximizers of the mutual information.
While there is consensus that such an approach leads to appropriate objective priors in low-dimensional cases, it is not always the case in high-dimensional settings.
Indeed, the prior maximizing the mutual information has often non-desirable properties in such scenarios, according to \citet{berger_overall_2015}. %They say that the actual maximizer of the mutual information is either too diffuse either too con
In \cite{berger_development_1992}, the authors detail their suggestion to define the reference priors hierarchically, by considering an ordering of the parameters:
 \begin{equation}\label{eq:intro-ref:hierartheta}
     \theta = (\theta_1,\dots,\theta_r) \in \Theta=\Theta_1\times\dots\times\Theta_r.
 \end{equation}
Typically, it is recommended to assume $\Theta_j\subset\RR^{d_j}$ with small dimensions $d_j$ (e.g., lower or equal than $2$) for any $j\in\{1,\dots,r\}$, and to sequentially build a reference prior on the $\Theta_j$,  $j\in\{1,\dots,r\}$:
 \begin{enumerate}
     \item initially fix $\ell_k^1=\ell_k$;
     \item for any values of $\theta_{j+1},\dots,\theta_r\in\Theta_{j+1}\times\dots\times\Theta_r$, compute a reference prior (in the sense of \cref{def:intro-ref:ref-priors})  under the model with likelihood $\theta_j\mapsto\ell_k^j(\mbf y|\theta_j,\dots,\theta_r)$, denote $\pi_j(\cdot|\theta_{j+1},\dots,\theta_r)$ its normalized density;
     \item derive $\ell_k^{j+1}$ such as 
         \begin{equation}\label{eq:hier:condlikeint}
            \ell_k^{j+1}(\mbf y|\theta_{j+1},\dots,\theta_r) =  \int_{\Theta_j}\ell_k^j(\mbf y|\theta_j,\dots,\theta_r)d\pi_j(\theta_j|\theta_{j+1},\dots,\theta_r).
         \end{equation}
 \end{enumerate}


This construction is often considered in the literature when it resorts to define reference priors in multidimensional context. 
However, it relies on the hierarchization of the parameter fixed in \cref{eq:intro-ref:hierartheta}, and the recommended  one depends on the model of interest.
There are a few models, such as the multinomial (defined in \cref{ex:intro-ref:multinomialmodel}) one, for which the different reference priors resulting from this construction have been extensively studied. In the case of the multinomial model, it is claimed that this construction with an ordering where $r=d$ when $\Theta\subset\RR^d$ must be favored  \citep{berger_ordered_1992,berger_overall_2015}.
\textcolor{orange}{In this case, the authors show that the density $\pi^\ast$ of this reference prior given by this construction is given by
    \begin{equation}\label{eq:intro-ref:multinomialhierarref}
        \pi^\ast(\theta) = K \left[\prod_{j=1}^r\prod_{i=1}^{d_j}\theta_j^{(i)-1/2} \right] \left[\prod_{j=1}^{r-1} \left(1- \sum_{j'=1}^j\sum_{i=1}^{d_j}\theta_{j'}^{(i)}   \right)^{-d_{j+1}/2}  \right] \left[1- \sum_{j=1}^r\sum_{i=1}^{d_j}\theta_j^{(i)} \right]^{-1/2},
    \end{equation}
where $K$ is a normalization constant.}





\subsection{Properties and Jeffreys prior}\label{sec:intro-ref:properties}


%The formulation of the reference prior in \cref{thm:intro-ref:explicitRP} comes from the maximization of the mutual information. The heuristic developed in \cite{bernardo_bayesian_1994} solve the optimization problem applying a Lagrangian theorem to maximize $\sI^k$ w.r.t. the prior density $\pi$, subject to the optimization constraint $\int\pi =1$.

%Beyond maximizing the mutual information, and so being a good candidate for being qualified as objective, the reference priors satisfy the  following properties.




The elucidation of the reference priors remains an open question for some restricted sets of priors $\cP$ or when we lack of regularity assumptions on the model.
%Generally, it is common 
In most studies, $\cP$ remains as large as possible and is restricted only by some mild regularity assumptions, such as that the priors admitting continuous densities.
%As we solve an optimization problem, \citet{mure_objective_2018} has pointed out that the class $\cP$ should be 
% The most general properties satisfied by the reference priors are recalled in \cite{berger_formal_2009}.
% \begin{prop}[Independence from sample size]
    
%     Theorem 2.19 (Independence from sample size). A reference prior for a model (Pθ)θ∈Θ  remains the same for the model (P ⊗n  θ )θ∈Θ regardless of n ∈ Z+.  Proof. This is because the definition of a reference prior relies on asymptotics.  Theorem 2.20 (Compatibility with sufficient statistics). If the observed data are restricted  to a sufficient statistic, reference priors remain unchanged
% \end{prop}



% \citet{berger_formal_2009} recall the following general properties of the reference priors.






In the general case, \citet{mure_objective_2018} studied the uniqueness of reference priors. 
His study requires to define the sequence $(\Theta_i)_i$ in \cref{def:intro-ref:ref-priors} ``appropriate'' if and only if the $\Theta_i$ are open subsets of $\Theta$.
%Considering the class of admissible priors 
%
We summarize his main result as follows:
\begin{thm}
    Let $\cP$ be the set of admissible priors. 
    Call an ``appropriate'' increasing sequence of subsets $(\Theta_i)_i$, an increasing sequence such that each for any $i$, $\Theta_i$ is open.
    Under the assumption that there exist a weakly estimator of $\theta$, the reference prior over $\cP$, if it exists, is unique up to a multiplicative constant.
\end{thm}

This result allows referring to ``the reference prior'' instead to ``reference priors'' in some cases. %However, it is limited to cases where a very general class $\cP$. %is considered
Also, there does not exist to our knowledge a result about the existence of the reference prior in the general case.
% That makes the study

Actually, the reference priors are mostly studied in what are called regular cases.
The following assumption is a regularity assumption on the statistical model, it ensures useful tools to be well-defined.
\begin{assu}\label{assu:intro-ref:jeffreysexist}
    $\Theta$ is a subset of $\RR^d$ with non-empty interior, and the likelihood is such that
    \begin{enumerate}
        \item for $\mu$-a.e. $y$, $\theta\mapsto\ell(y|\theta)$ is twice differentiable, with $\nabla^2_\theta\ell(y|\theta)$ being negative definite,
        \item for any $\theta\in\Theta$, there exists $\tau>0$ %and $K>0$ 
        such that %for any $\theta\in B(\hat\theta,\tau)$
        \begin{equation}
            \EE_{Y\sim\PP_{Y|\theta}}\left[\sup_{\theta',\,\|\theta-\theta'\|<\tau} \|\nabla^2_\theta\log\ell(Y|\theta')\| \right]\quad\text{and}\quad \EE_{Y\sim\PP_{Y|\theta}}\left[\sup_{\theta',\,\|\theta-\theta'\|<\tau}\|\nabla_\theta\log\ell(Y|\theta')\|^2\right],
        \end{equation}
        are continuous functions of $\theta$ on the interior of $\Theta$.
        %are  bounded w.r.t. $\theta\in B(\hat\theta,\tau)$.
    \end{enumerate}
\end{assu}

Under \cref{assu:intro-ref:jeffreysexist}, the Fisher information matrix and the Jeffreys prior are well-defined (see \cite{lehmann_elements_1999}). The Fisher information matrix $\cI$ is expressed as
    \begin{equation}
        \cI(\theta) = (\cI(\theta)_{i,j})_{i,j=1}^d\quad \text{with}\quad \cI(\theta)_{i,j}= \int_\cY[\partial_{\theta_i}\log\ell(y|\theta)]
        [\partial_{\theta_j}\log\ell(y|\theta)]\, \ell(y|\theta)d\mu(y),
    \end{equation}
under \cref{assu:intro-ref:jeffreysexist} its coefficients can be expressed as follows
    \begin{equation}
       \cI(\theta)_{i,j}= - \int_\cY[\partial^2_{\theta_i\theta_j}\log\ell(y|\theta)]\, \ell(y|\theta)d\mu(y).
    \end{equation}
The Jeffreys prior refers to the prior whose density is $J$, defined by
    \begin{equation}
        J(\theta) = \sqrt{\det\cI(\theta)}.
    \end{equation}
Under \cref{assu:intro-ref:jeffreysexist}, $J$ is a continuous and positive function on $\Theta$. \textcolor{orange}{Here are a few examples of Jeffreys priors in different modelings.}

\color{orange}

\begin{ex}[A proper Jeffreys prior in the multinomial case]
    The multinomial model is defined as $\theta\in(0,1)^d$, $\cY=\{y\in\{0,\dots,n\}^d,\,\sum_{i=1}^dy^i=n\} $ for a fixed $n\in\NN^\ast$.
    And with 
        \begin{equation}
            \ell(y|\theta) = \frac{n!}{y^1!\dots y^d!}\prod_{i=1}^d\theta_i^{y^i}.
        \end{equation}
    In this model, the Jeffreys prior corresponds to a Dirichlet($1/2,\dots,$ $1/2$) distribution:
        \begin{equation}
            J(\theta)\propto \prod_{i=1}^d\theta_i^{-1/2}(1-\theta_i)^{-1/2}.
        \end{equation}
    It is a proper prior and corresponds to the prior given in \cref{eq:intro-ref:multinomialhierarref} when $r=d$.  
\end{ex}

\begin{ex}[Jeffreys prior in the Gaussian case]
    The Gaussian model is defined by $\cY=\RR$, $\ell(y|\theta) = \frac{e^{-\frac{(x-\mu)^2}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}} $, with $\theta=\mu\in\RR $ in the known variance case, or $\theta=(\mu,\sigma^2)\in\RR\times(0,\infty) $ in the unknown variance case.
    In both cases, the Jeffreys prior has a density:
        \begin{equation}
            J(\theta)\propto \frac{1}{\sigma}.
        \end{equation}    
    In the known variance case it corresponds to a uniform distribution on $\RR$. In both cases, it is improper.  However, its posteriors are always proper.
\end{ex}

\color{black}


\citet{clarke_jeffreys_1994} have shown that the Jeffreys prior  is the reference prior over the priors with continuous and positive densities on $\Theta$. Their result can be expressed as follows.
    \begin{thm}\label{thm:intro-ref:clarke}
        Under \cref{assu:intro-ref:jeffreysexist}. Call an ``appropriate'' increasing sequence of set $(\Theta)_i$, an increasing sequence such that for any $i$ $\Theta_i$ is compact. Assume $\nu$ is the Lebesgue measure and let $\cP$ be the set of priors admitting a continuous and positive density. Then the Jeffreys prior is the unique reference prior over $\cP$.
    \end{thm}
\citet{mure_objective_2018} has shown that the above theorem still stands when ``appropriate'' refers to open subsets of $\Theta$. \textcolor{orange}{This result can be anticipated by reminding that the form of the density maximizing the mutual information for a fixed $k$ is $\exp\left(\int_{\cY_k}\ell_k(\mbf y|\theta)\log(p(\theta|\mbf y) ) d\mu^{\otimes k}(\mbf y)\right)$. 
Indeed, according to the Bernstein von Mises theorem (see e.g. \cite{van_der_vaart_asymptotic_1992}), under appropriate assumptions, the posterior density $p(\theta|\mbf y)$ is asymptotically close  to a %converges to a 
centered Gaussian density whose  variance is $\cI(\theta)^{-1/2}/\sqrt{k}$ when $\mbf y$ are sitributed w.r.t. $\PP_{\mbf Y_k|\theta}$.
%where $\hat\theta_k$ is} an asymptotically sufficient and consistent estimator, such as the {Maximum Likelihood Estimator} (MLE) under general assumptions. 
This convergence does not depend {on} the prior. We thus asymptotically have
    \begin{equation}
        \log p(\theta|\mbf y)\simeq \log \frac{k}{2\pi}  +\frac{1}{2}\log\det\cI(\theta),
    \end{equation}
and we approximate
\begin{equation}
    \int_{\cY^k} \ell_k(\mbf y|\theta)\log p(\theta|\mbf y) d\mu^{\otimes k}(\mbf y)\simeq \log\frac{k}{2\pi} + \frac{1}{2}\log\det\cI(\theta).
\end{equation}
Calling $f_k(\theta) = \exp\left(\int_{\cY^k}\ell_k(\mbf y|\theta)\log(p(\theta|\mbf y))d\mu^{\otimes k}(\mbf y)  \right)$, the above heuristic gives that $f_k$ is asymptotically proportional to $\exp\left(\frac{1}{2}\log\det\cI(\theta)\right)$, which is the density of the Jeffreys prior.}



We conclude this section by noticing that
the Jeffreys prior arises as a cornerstone in the reference prior theory. We recall that
this prior has been introduced by \citet{jeffreys_invariant_1946}. It is a prior that is already praised in Bayesian statistics for its main property to be invariant by re-parametrization of the model:
define $\phi=g(\theta)$ with $g$ being a diffeomorphism, and define the model induced considering $\phi$ instead of $\theta$. Then the Jeffreys prior for this re-parameterized model admits the density $J_g$:
    \begin{equation}
        J_g(\phi) = J(g^{-1}(\phi)) %\left|\frac{dg^{-1}}{d\phi} (\theta)\right| 
        |\det d_\phi g^{-1}(\phi)|.
    \end{equation}
This ensures that inference is not affected by the choice of parameterization, preserving coherence.
We also recall that the Jeffreys prior is known to be improper in most cases.


%While the above let

%\Cref{thm:intro-ref:clarke} tempts to  limit the study to the one of the Jeffreys prior of the considered model.
%The Jeffreys prior has been introduced by \citet{jeffreys_invariant_1946}. It is a prior that is already praised in Bayesian statistics for its main property to be invarient by reparametrization of the model













%\section{The concept of objective priors and the role of mutual information}

%\section{Reference prior definition and properties}

\section{Limits of the classical framework of reference priors and solution}\label{sec:intro-refs:limits}


%The reference prior definition is complex as

During this brief review, we mentioned some limits in the framework of the reference priors definition.
They are related to the incorporation of improper priors in the admissible priors. As a matter of fact, when dealing with non-informative distributions, it is common to find one of the main assumptions on probability distributions (that is, their total mass is $1$) limiting.
The fact is that in some cases, improper distribution seems really ``close'' to proper ones.
As an example, we could mention that the improper uniform distribution on $\RR$ can be seen as a Gaussian distribution with a variance that diverges to infinity. A logarithmic re-parametrization of the latter gives the improper distribution of density proportional to $x\mapsto1/x$.

Additionally, when dealing with densities, nothing strongly forbid the definition of the posterior density when the prior is improper (see \cref{sec:intro-ref:improperway-around}). It became therefore common to normalize the use of improper prior, and to carry on \emph{a posteriori} inference from them when the issued posterior is proper.

Even though the way-around works, we lose the existence of a latent probability space with random variables expressing formally the existence of a path that links the ``chaos'' of a universe to the phenomena observed and modeled.
This section is dedicated to the elucidation of such construction, in a framework that allows priors to be improper.



\subsection{Novel framework for Bayesian inference with improper priors}\label{sec:intro-ref:novelframework}



%The purpose of statistics is to observe real phenomena, and to construct their interaction with probability theory. Thus, s
Since the early work of \citet{kolmogorov_foundations_1933}, it is appreciated to be reminded that there exists an abstract, underlying structure from which result the probabilistic description and tools that are used to perform inference. This structure, which takes the form of a probability space in the axiomatic of Kolmogorov, represents the indescribable phenomena that induce randomness in the observations.

However, the usual Kolmogorov's axiomatic does not take  into account improper distributions to construct the underlying space that provides the usual statistical modeling. 
In this section, we
construct from scratch a framework for Bayesian statistics, in a way that allows priors to be improper.
We re-define the objects defined in \cref{sec:intro-ref:classical-framework}.

%when we work with probabilistic tools,


%The Bayesian framework is standardly defined from a collection of probability measures $(\PP_{Y|\theta})_{\theta\in\Theta}$ that constitutes a statistical model, alongside with a prior $\pi$ on $\Theta$ ---namely, a probability distribution---. It is therefore possible to construct a probability space $\mbf\Omega$ that models the observations: they are modeled as the independent realizations of a random variable $Y\in\cY$, with $Y$ being distributed according to $\PP_{Y|\theta}$ conditionally to $T=\theta$ where $T$ is a random variable with distribution  $\pi$.  

%However, it is common when it resorts to Bayesian analysis to allow a construction based on an `improper' prior, namely, a prior defined from a $\sigma$-finite measure, whose total mass is not necessarily finite. 
%Their consideration rests essential within the reference prior theory because %, as it will be developed later on, 
%the reference priors are known to be often improper.
%It is not singular to allow such priors ---even though it is recommended that they are limits\footnote{Different authors in the literature propose different ways to define such limits; a review of them is not the purpose of this study.} of proper priors--- 
%While it is not singular to allow such prior %---on the condition they --- when they issue proper posterior distribution,

%A solution for this purpose is often to expect that the prior is at least a limit of proper priors and to refer to 

%In this section we review 
Our suggestion is based on the work of \citet{taraldsen_conditional_2016}, who adapted the propositions of \citet{renyi_foundations_1970}. Their idea is to extend the notion of a probability space when the probability is only a $\sigma$-finite measure. Up to a multiplicative constant, their framework is shown to coincide with standard probability spaces when the measure has a finite total mass, or when conditioning on a set with finite mass.
%Below is given a construction for a definition of a prior.


We let $(\Theta,\sT)$ be a measurable space. If $\varPi_1$ and $\varPi _2$ are two non-null, non-negative and $\sigma$-finite  measures we can define the relation $\simeq$ by
    \begin{equation}
        \varPi_1\simeq\varPi_2\Longleftrightarrow\exists t>0,\,\varPi_1= t\varPi_2.
    \end{equation}
Define by $\sM$ the space of non-null, non-negative and $\sigma$-finite measures on $\Theta$. The following properties are verified:
    \begin{itemize}
        \item The relation $\simeq$ is an equivalence relation on $\sM$. We note $[\varPi]$ the set of a $\varPi\in\sM$.
        %\item If $(\Theta,\sT)$ is a Borel space, then $\widehat\sM$, the space of the non-null non-negative Radon measures, is stable by $\sim$, i.e., $\pi\in\widehat\sM\Longrightarrow[\pi]\subset\widehat\sM$. %It implies that $\sim$ is an equivalence relation on  and $\hat\sM$ with $\hat\sM/\!\simeq\,\subset\sM/\!\simeq$.
        \item Denote by $\sM^\nu$ the space of absolutely continuous measures w.r.t. $\nu\in\sM$. It is stable by $\simeq$.
        \item Define by $\sR$ the space of non-negative non-null measurable functions from $\Theta$ to $\RR$. The relation%Let $\nu\in\sM$, the relation $\propto$ defined by
            \begin{equation}
                f\propto g \Longleftrightarrow\exists t>0,\,f= t g%\ \ \text{$\nu$-a.e.}
            \end{equation}
        is an equivalence relation on $\sR$. %, the quotient space is denoted by $\sR/\!\propto$.    
        Let $\nu\in\sM$, the Radon-Nikodym Theorem defines a natural surjection from the quotient space $\sR/\!\propto$ to $\sM^\nu/\!\simeq$. 
        A class in $\sR/\!\propto$ is called a density class of the unique class in $\sM^\nu/\!\simeq$ that it induces by this mapping. If two elements $[f]$, $[g]$ of $\sR/\!\propto$ are density classes of a same element in $\sM^\nu/\!\simeq$, they are equal $\nu$-a.e. in the sense $\exists t>0$, $f= t g$ $\nu$-a.e.
        %the quotient space of $\sR/\!\propto$ by the relation of equaity $\nu$-a.e. is in bijection with $\sM^\nu/\!\simeq$. For a class in $\sM^\nu/\!\simeq$, its associated class in $\sR/\!\propto$ is called its density class w.r.t. $\nu$.
    \end{itemize}

\citet{bioche_approximation_2016} consider such a definition of $\sigma$-finite measures equal up-to-a constant to formalize the notion of prior and posterior approximations, whether they are proper or improper. Notably, they define a topology induced by a convergence in $\sM/\!\simeq$: the Q-vague convergence. %provide a topology on $\sM/\!\simeq$ baed on  %They show that $\sM/\!\simeq$


A prior is then generally defined as a class $[\varPi]$ in $\sM/\!\simeq$.
%It corresponds to what \citet{Taraldsen2016} call a $C$-measure: a measure defined up to a constant. 
The tuple $(\Theta,\sT,[\varPi])$ corresponds to what \citet{taraldsen_conditional_2016} call a conditional probability space for any $[\varPi]\in\sM/\!\simeq$.
For any $U \in \sT$ such that 
$\varPi(U)\in (0,+\infty)$, 
%$\pi(U) < +\infty$, 
that space issues a unique probability space on $U$, with the probability $\varPi(\cdot|U):=\varPi(\cdot\cap U)/\varPi(U)$, it is independent of the choice of the representative of $[\varPi]$.
This framework gives what is necessary to construct a general conditional probability space that models our problem, as we express below.


\begin{prop}\label{prop:intro-ref:kolmog}
We assume that the observations take values in a 
Polish space $(\cY,\sY)$ and are statistically modeled by the collection of conditional probabilities $(\PP_{Y|\theta})_{\theta\in\Theta}$ with $(\Theta,\sT)$ being a Polish space and such that $\forall A\in\sY$, $\theta\mapsto\PP_{Y|\theta}(A)$ is measurable. We consider $\varPi\in\sM$.\\
Then there exist a conditional probability space $(\mbf\Omega, \mbf\Xi, [\mbf\Pi])$, a measurable process $\overline Y=(Y_i)_{i\in\NN}$, $Y_i:\mbf\Omega\to\cY$, and a measurable function $T:\mbf\Omega\to\Theta$ such that
    \begin{enumerate}
        \item for any $U\in\sT$, $\mbf\Pi(T\in U)=\varPi(U)$; 
        \item if 
        $\varPi(U) \in  (0,+\infty)$ 
        %$\pi(U)<+\infty$ 
        then the unique probability space conditioned on $\{T\in U\}$ is such that the conditional probability of any $(Y_{i_l})_{l=1}^k$ to $T=\theta$ is $\PP_{Y|\theta}^{\otimes k}$. More explicitly, if $A_{j_1},\dots,A_{j_k}\in\sY$ then
            \begin{equation}
                \mbf\Pi\Big(\bigcap_{l=1}^kY_{j_l}\in A_{j_l}|T\in U\Big) = \int_\Theta\PP_{Y|\theta}^{\otimes k}(A_{j_1}\times\dots\times A_{j_k})d\varPi(\theta| U).
            \end{equation}
    \end{enumerate}
\end{prop}

%Such existence of the underlying abstract space $\mbf\Omega$ provides the desired interpretability of the modeling. It can be seen as the source of the uncontrollable randomness that goes beyond the observations and the parameter. It is proven in Section \ref{sec:intro-ref:proof}. One can notice that it is consistent with usual Bayesian frameworks when $\varPi$ is proper.

\Cref{prop:intro-ref:kolmog} is proven in \cref{sec:intro-ref:proof}. It ensures the existence of an underlying abstract space $\mbf\Omega$
from only the given of the statistical model (the collection of probability measures $(\PP_{Y|\theta})_{\theta\in\Theta}$) and the prior (which is a measure $\varPi\in\sM$). When $\varPi$ is improper, the  tuple $(\mbf\Omega,\mbf\Xi,[\mbf\Pi])$ is not a probability space anymore, %neither $T$ is a random variable, 
but they still describe the randomness that goes beyond the observations and the parameter.
In the following section, we develop the compatibility of this framework with the usual one.







%Actually, either a measure has a finite total mass, in which case it can easily be turned into one 




\subsection{Notations and compatibility with the usual framework}\label{sec:intro-ref:frameworkcompatibility}


Given the construction proposed in \cref{sec:intro-ref:novelframework}, the Bayesian framework is defined from the given of probability measure $(\PP_{Y|\theta})_{\theta\in\Theta}$ and a prior $\varPi$ which is a representative of an element $[\varPi]\in\sM/\!\simeq$, under the assumptions in the \cref{prop:intro-ref:kolmog}.
%It is important to remind that the construction 
The consideration of another representative $\tilde\varPi$ of  $[\varPi]$ does not alter the construction.


%First, notice that when the prior is proper ($\varPi(\Theta)<\infty$), 

That framework allows the construction of a marginal distribution $\PP_{\mbf Y_k}$, defined as the pushforward measure of $\mbf\Pi$ by $\mbf Y_k$.
In the same way as $\varPi$, it is unique up to a constant.
The posterior distribution $\PP_{T|\mbf y}$ is well-defined for $\PP_{\mbf Y_k}$-a.e. $\mbf y$ by
\begin{equation}
    \forall A\in\sY^{\otimes k},\,\forall B\in\sT,\, \int_{A}\PP_{T|\mbf y}(B)d\PP_{\mbf Y_k}(\mbf y) = \int_B \PP_{\mbf Y_k|\theta}(A)d\varPi(\theta).
\end{equation}
It belongs to $\sM$, and its class $[\PP_{T|\mbf y}]$  in $\sM/\!\simeq$ is uniquely defined.


\subsubsection{Densities}

From now on, we assume that the model admits a likelihood: there exists a collection of density functions $(\ell(\cdot|\theta))_{\theta\in\Theta}$  w.r.t. a common  $\sigma$-finite measure $\mu$ on $\cY$ such that for $\varPi$-a.e. $\theta$:
\begin{equation}
    \forall A\in\sY,\,\PP_{Y|\theta}(A) = \int_A\ell(y|\theta)d\mu(y).
\end{equation}
We also assume that $\varPi$ belongs to $\sM^\nu$ (it is absolutely continuous w.r.t. $\nu$), with $\nu\in\sM$. We denote by $\pi\in\sR$ a density of $\varPi$ w.r.t. $\nu$.
For $\mbf y\in\cY^k$ there exists a posterior density w.r.t. $\nu$ defined in  the class $[p(\cdot|\mbf y)]\in\sR/\!\propto$ where
    \begin{equation}
        p(\theta|\mbf y) 
        \propto  \ell_k(\mbf y|\theta)\pi(\theta) = \prod_{i=1}^k\ell(y_i|\theta)\pi(\theta)
    \end{equation}
If the posterior is proper for $\PP_{\mbf Y_k}$-a.e. $\mbf y$, a marginal density can be defined $\PP_{\mbf Y_k}$-a.e. by
    \begin{equation}
        p_{\mbf Y_k}(\mbf y) =  \int_\Theta\ell_k(\mbf y|\theta)\pi(\theta)d\theta.
    \end{equation}


\subsubsection{Impact on the framework of reference priors}

The elucidating of the densities in this novel Bayesian construction is consistent with the usual construction.
When restricting the framework to a $\tilde\Theta\in\sT$ such that $0<\varPi(\tilde\Theta)<\infty$, the restricted model driven by $\tilde\Theta$ (in the sense given in \cref{sec:intro-ref:improperway-around})  corresponds to the model on $\tilde\Theta$ considering the prior $\varPi(\cdot|\tilde\Theta)$.

Therefore, the notations and definitions written in the previous sections still stand.
The modeling remains the same for any prior that equals  $\varPi$ up to a positive multiplicative constant, so that if $\varPi\in\sM$ is a reference prior, any prior in $[\varPi]$ also is.
For this reason it becomes appropriate to talk about a reference prior class. 
We will authorize however the slight abuse of language referring to ``the reference prior'' instead of ``reference prior in the reference prior class'', when the latter is unique.
We formalize this terminology in the following definitions.

\begin{defi}
    A set of priors refers to a subset of $\sM^\nu$.\\
    When $\nu$ is the Lebesgue measure, a set of continuous prior refers to a subset $\cP\subset\sM^\nu$ where for any $\varPi\in\cP$, $\varPi$ admits a $\nu$-a.e. continuous density $\pi$. We call $\sM_\cC^\nu$ the union of the sets of continuous priors.\\
    The subset of $\sR$ composed by $\nu$-a.e. continuous functions is denoted $\sR_\cC$.
\end{defi}

\begin{prop}
    $\sM_\cC^\nu$ is stable by $\simeq$ and $\sR_\cC$ is stable by $\propto$.\\
    If $\Theta\subset\RR^d$, and if $f\in\sR_\cC$, there exists $g$ that equals $f$ $\nu$-a.e. and that is locally bounded on $\Theta$.\\
    The subset of $\sR$ composed by locally bounded and $\nu$-a.e. continuous functions is denoted $\sR_{\cC^b}$, it is stable by $\propto$ and any $f$ in $\sR_\cC$ is bounded on every compact.
\end{prop}

\begin{proof} 1.
% \begin{enumerate}
%     \item 
The stability of $\sR_\cC$ and $\sR_{\cC^b}$ to $\propto$ comes since multiplying a function by a constant does not affect its continuity. \\
We consider $M$ the mapping assigning a density in $\sR$ to its associated measure in $\sM^\nu/$ as
\begin{equation}
    M:f\in\sR \longmapsto \left(B\in\sT\mapsto \int_Bf(\theta)d\nu(\theta) \right).
\end{equation}
We have $f\propto g\implies M(f)\simeq M(g)$, so that $M(\sR_\cC) $ is stable by $\simeq$. The second statement of the proposition comes since $M(\sR_\cC)=\sM^\nu_\cC$ by definition.\\
2. For the second statement, we consider $f\in\sR_\cC$
We note $E=\{\theta,\, \forall\tau>0,\, f$ is not bounded on $B(\theta,\tau) \} $.
Let $\theta\in E$, 
for any $n>0$, $f$ is not bounded on $B(\theta,1/n)$, so that there exists $\theta_n\in B(\theta,1/n)$ such that $f(\theta_n)>n$. That leading to $f(\theta_n)\conv{n\rightarrow\infty}\infty$ while $\theta_n\conv{n\rightarrow\infty}\theta$, $f$ is discontinuous on $\theta$. 
We deduce that $\nu(E)=0$. Thus, defining $g$ by $g=f$ on $\Theta\!\setminus\! E$ and $g=0$ otherwise ensures $g$ is in $\sR_\cC$ and is locally bounded, with $g=f$ $\nu$-a.e. \\
3. If $\tilde\Theta$ is a compact subset of $\Theta$, we can cover $\tilde\Theta$ by open balls on which $f$ is bounded (consequently to statement~2.). Since $\tilde\Theta$ is compact, the open cover admits a finite  subcover. $f$ being bounded on this finite subcover, we deduce that it is bounded on $\tilde\Theta$.
% \item 
% \end{enumerate}
\end{proof}


\begin{defi}[Advanced definition of reference priors]
    Let $\cP$ be a set of priors. A prior $\varPi\in\cP$ is a reference prior over $\cP$ if there exists an openly increasing sequence of compact sets $(\Theta_i)_{i\in\NN}$ such that $\bigcup_{i\in\NN}\Theta_i=\Theta$ with for any $i$: $0<\varPi(\Theta_i)<\infty$ and
    \begin{equation}
        \lim_{k\rightarrow\infty}\sI^k(\varPi(\cdot|\Theta_i)) - \sI^k(P(\cdot|\Theta_i))  \geq 0 \text{\ for all\ } P\in\cP\text{\ verifying\ }0<P(\Theta_i)<\infty.
    \end{equation}
    Let $\cQ$ be a subset of $\sM^\nu/\!\simeq$.
    And denote by $q$ the surjection $q:\varPi\in\sM^\nu\mapsto[\varPi]$.  
    A class $[\varPi]\in\cQ$ is called a reference prior class over $\cQ$ if $\varPi$ is a reference prior over $q^{-1}(\cQ)$.
\end{defi}



The above definition is congruous with the \cref{def:intro-ref:ref-priors}, apart from the necessary condition on the sequence $(\Theta_i)_i$. 
The definition of an openly increasing sequence of compacts
is stated below. It is, according to ourselves, an appropriate condition on this sequence, as it allows the consistency between the compact case and the general case.


\begin{defi}
    A sequence $(\Theta_i)_{i\in \NN}$ of subsets of $\Theta$ is said to be openly increasing if there exist $i_0\geq0$ and a sequence $(V_i)_{i\geq i_0}$ of open subsets of $\Theta$ such that for any $i\geq i_0$:
    \begin{equation}
        \Theta_i\subset V_i\subset\Theta_{i+1}.
    \end{equation}
\end{defi}

\color{orange}
Such sequence does not necessarily exist if $\Theta$ has an ``unusual'' form. 
In most cases, one would work on sets $\Theta$ that are ``almost'' open or ``almost'' closed, i.e. $\nu(\partial\Theta)=0$. In this case, one can restrict the study to $\overline{\Theta}$ or to $\mathring{\Theta}$. If $\Theta$ is moreover a subset of $\RR^d$ then there always exist openly increasing sequence of compact subsets $(\Theta_i)_{i\in\NN}$, $(\Theta'_i)_{i\in\NN}$ with $\bigcup_{i\in\NN}\Theta_i=\mathring{\Theta}$ and $\bigcup_{i\in\NN}\Theta_i'=\overline{\Theta}$. We express that statement in the proposition below.


% \color{orange}
\begin{prop}
     If $\Theta$, is an open or a closed subset of $\RR^d$ there exist an openly increasing sequence of compact subsets $(\Theta_i)_{i\in\NN}$ such that $\bigcup_{i\in\NN}\Theta_i=\Theta$.
\end{prop}

\begin{proof}
    If $\Theta$ is open,
    then we can write $\Theta$ as an increasing countable union of open balls: %that are strictly included in $\Theta$:
        \begin{equation}
            %\Theta &= \bigcup_{\substack{\theta\in\Theta\cap\QQ^d\\ \tau\in R(\theta)}} B(\theta,\tau) \quad\text{with}\quad R(\theta)=\{\tau\in\QQ_+^\ast,\, B(\theta,\tau)\subset\Theta \}\\
            \Theta = \bigcup_{n\in\NN^\ast}\bigcup_{\theta\in\Theta\cap\RR^d} B(\theta, \min(R(\theta)-\frac{1}{n},n) ) \quad \text{with} \quad R(\theta) = \sup\{\tau>0,\,B(\theta,\tau)\subset\Theta\}. 
        \end{equation}
    We write $\Theta\cap\QQ^d=\{\theta_i,\,i\in\NN^\ast\}$, and write $\Theta=\bigcup_{n\in\NN^\ast}V_n$ with $V_n=\bigcup_{i=1}^nB(\theta_i,\hat R^i_n)$, where $\hat R^i_n =\min(R(\theta_i)-\frac{1}{n},n)$. This way, each $V_n$ is a finite union of closed balls, so that $\overline{V_n}$ is compact. Also, $V_{n+1}$ contains balls with the same center as the one that constitute $V_i$, yet with higher radius. Thus, $\overline{V_n}\subset V_{n+1}$. Therefore, setting $\Theta_i=\overline{V_i}$ for all $i$ makes $(\Theta_i)_{i\in\NN^\ast}$ being an openly incresing sequence of compact subsets that cover $\Theta$.

    If $\Theta$ is closed, %if it compact then the results comes taking $\Theta_i=V_i=\Theta$ for all $i$. If it is not compact
    we recall that $\cF=\{\Theta\cap B(\theta,\tau),\,\theta\in\QQ^d,\,\tau\in\QQ^\ast_+\} $ is a subbase  of the induced topology of $\RR^d$ on $\Theta$.
    %Since $\Theta$ is not compact, one can apply Alexander subbase theorem (see e.g. RUDIN??): there exist a cover of $\Theta$ by elements in $\cF$ from which one cannot extract a finite subcover. 
    We can thus cover $\Theta$ by elements in $\cF$:
    % We write this cover as 
    $\Theta=\bigcup_{i\in I}U_i$. It is possible to index this cover by $\NN$. We  now construct the sequences $(\Theta_i)_i\in\NN$ and $(V_i)_{i\in\NN}$ by induction. Starting from $V_0=U_0$ and $\Theta_0=\overline{V_0}$, we then take for all $i$, $V_{i+1}=\bigcup_{j=1}^{m(i)}U_j $, where $m(i) = \min\{j',\, \bigcup_{j=0}^{j'}U_j \supset {\Theta_i} \}$. %\in\NN\cup\{\infty\}$. 
    By induction, ${\Theta_i}$ is compact, so that $m(i)<\infty$ and $\Theta_{i+1}=\overline{V_{i+1}}$ also is.
%    We notice that if $m(i)=\infty$, then $V_{i+1}=\Theta\supset\overline{V_{i}}$, which remains an open subset of $\Theta$. Also, since $\Theta$ is closed, every $\Theta_i$ are compact subsets of $\Theta$. 
That makes $(\Theta_i)_{i\in\NN}$ being an openly increasing sequence of subsets that covers $\Theta$.
    % It is possible to index this cover by $\NN$ and to consider that it is strictly increasing. Doing so, the $V_i$ are not necessarily in $\cF$ anymore but are finite unions of elements in $\cF$.  Thus, for all $i$, $\Theta_i=\overline{V_i}$ is a finite union of closed balls, so that is it a compact. Since $\Theta$ is closed, $\Theta_i$ is a subset of $\Theta$
    %  Moreover, $\overline{V_i}\subset\overline{V_{i+1}}$    % The result comes from noticing that the family $\cF=\{V=B(\theta,\tau)\cap\Theta,\,\theta\in\QQ^d,\,\tau\in\QQ_+^\ast$ with $\overline{V}\subset\Theta\}$ is a subbase of the induced topology of $\RR^d$ on $\Theta$.
\end{proof}

% \begin{proof}
%     We write $\Theta=\bigcap_{j\in\NN}U_i$ for some open sets $U_i$.


%     If $\Theta$ is compact then the results follows by taking $\Theta_i=V_i=\Theta$ for all $i\in\NN$. Otherwise, we start by stating that the family of open subsets of $\Theta$ $\cF=\{V=B(\theta,\tau)\cap\Theta,\,\theta\in\QQ^d,\,\tau\in\QQ_+^\ast$ with $\overline{V}\subset\Theta\}$ is a subbase of the induced topology of $\RR^d$ on $\Theta$.
%     Indeed, let $\theta\in\Theta$, $\tau>0$, it is possible to write $B(\theta,\tau)\cap\mathring{\Theta}$ as a countable union of elements in $\cF$: $B(\theta,\tau)\cap\mathring{\Theta}=$
%     %Indeed, let $\theta\in\Theta$, $\tau>0$, and call $V={B}(\theta,\tau)\cap\Theta$. If $\overline{V}\subset\Theta$ then ok. Else,    %if %$\theta\in\mathring{\Theta}$ such that $\overline{B}(\theta,\tau)\cap\overline{\Theta}\not\subset\Theta $.
%     One can apply Alexander subbase theorem (see e.g. RUDIN??): there exist a cover of $\Theta$ by elements of $\cF$ from which one cannot extract a finite subcover. We write this cover as $\Theta=\bigcup_{i\in I}V_i$. Since $\cF$ is countable, it is possible to index the sequence $(V_i)_i$ by $\NN$: $\Theta=\bigcup_{i\in I}V_i$. %We can also assume that $(V_i)_i$ is an increasing sequence.  Indeed if it is not, 
%     We construct by induction an increasing cover of $\Theta$: first we define $(\tilde V_j)_{j\in\NN}$ from $\tilde V_0=V_0$ and for all $j$, $\tilde V_{j+1}=\bigcup_{i=1}^{m(j)} V_i$ where $m(j)=\min\{m,\,\bigcup_{i=1}^mV_i\ne \tilde V_i \} $. If $m(j)$ was infinite for some $j$, then $(V_i)_{i=0}^j$ would constitute a finite subcover of $\Theta$, which is a contradiction with Alexander subbase theorem. %This construction is licit while $m(j)$ is never infinite, which is the case since 
%     Now let $i\in\NN$, as $\tilde V_i\subset \tilde V_{i+1}$ with $\tilde V_i\ne\tilde  V_{i+1}$, 
% \end{proof}


\color{black}


\begin{prop}
    Assume $\Theta$ is compact. Then if $\varPi$ is a reference prior over a set of priors $\cP$, it verifies
        \begin{equation}\label{eq:intro-ref:refcompact}
            \lim_{k\rightarrow\infty} \sI^k(\varPi)-\sI^k(P) \geq0\text{\ for all\ } P\in\cP.
        \end{equation}
\end{prop}


\begin{proof} 
We consider an openly increasing sequence of compact sets $(\Theta_i)_{i\in\NN}$ such that $\bigcup_{i\in\NN}\Theta_i = \Theta$.
By definition there exist $i_0\geq0$ and a sequence $(V_i)_{i\geq i_0}$ of open subsets of $\Theta$  such that for any $i\geq i_0$
    \begin{equation}
        \Theta_i\subset V_i\subset \Theta_{i+1}.
    \end{equation}
This way, $\bigcup_iV_i=\Theta$ and the compacity of $\Theta$ imposes it to  be a finite union, so that $\Theta_i=\Theta$ for any $i\geq i_1$ for some $i_1\geq0$.
Since $\varPi$ is a reference prior over $\cP$ its renormalized restriction to $\Theta_{i_1}$ verifies the \cref{eq:intro-ref:refcompact}. Hence the result.
\end{proof}


\subsection{Proof of \cref{prop:intro-ref:kolmog}}\label{sec:intro-ref:proof}



Let $1\leq\mbf i\leq\infty$, $I=\{i\in\NN,\,i<\mbf i\}$, and $(U_i)_{i\in I}$ be a sequence of disjoint non-empty sets in $\sT$ that covers $\Theta$ such that $\varPi(U_i)<\infty$ $\forall i$.
When $\varPi(U_i)=0$, we denote by $\varPi(\cdot|U_i)$ an arbitrary probability distribution on $U_i$ (for instance a Dirac distribution); otherwise, it is defined by $\varPi(\cdot|U_i)=\varPi(\cdot\cap U_i)/\varPi(U_i)$.
    Fix firstly $i\in I$ and for any $k\geq1$ let us construct $\PP_{\mbf Y_k,T}^i$ on $\sY^{\otimes k}\otimes\sT$ such that
        \begin{equation}
            \PP_{\mbf Y_k,T}^i(A_1\times\dots\times A_k\times B) = \int_B \PP_{Y|\theta}^{\otimes k}(A_1\times\dots\times A_k)d\varPi(\theta|U_i),
        \end{equation}
     so that $\PP^i_{\mbf Y_k,T}$ is a probability distribution on $\cY^k\times U_i$. 
    Therefore, using Kolmogorov extension theorem, there exist a probability space $(\Omega^i,\sX^i,\PP^i)$, a random process $\overline{\mbf Y^i}=(Y^i_j)_{j\geq1}$, $Y_j^i:\Omega^i\to\cY$, and a random variable $T^i:\Omega^i\to\Theta$ such as  their joint distribution is uniquely defined by the distribution
    %$(T^i,\mbf Y_k^i)\sim
    $\PP^i_{\mbf Y_k,T}$ of $(T^i,\mbf Y_k^i)$ for any $k$, with $\mbf Y_k^i=(Y_j^i)_{j=1}^k$.



%\color{red}
Denote $\mbf\Omega= I \times\prod_{i\in I}\Omega^i$. A natural $\sigma$-algebra on $\mbf\Omega$ is the one generated by the cylinder sets:
\begin{equation}
    \mbf\Xi := \sigma\big(N\times E_0\times\dots\times E_{k-1}\times\prod_{k\leq i<\mbf i}\Omega^i,\,N\in\sP(I),\, E_0,\dots,E_k\in\sX^i,\,k<\mbf i \big).
\end{equation}
We can define $\overline\varPi$ by
    \begin{equation}
        \overline\varPi\Big(N\times\prod_{i\in I}E_i\Big) = \sum_{n\in N}\PP^n(E_n)\varPi(U_n),
    \end{equation}
for any $N\in\sP(I)$, $(E_i)_{i \in I} \in\prod_{i\in I}\sX^i$ with $E_i=\Omega^i$ for any $i\geq k$ for some $k<\mbf i$. 
The existence of a measure $\mbf\varPi$ on $\mbf\Xi$, that coincides with $\overline\varPi$ on the cylinder sets is guaranteed by  Carathéodory's extension Theorem. %$^\ast$




Now, we call $p^n:(m,(\omega^i)_i)\in\mbf\Omega\mapsto \omega^n$ for any $n\in I$, $c:(m,(\omega^i)_i)\in\mbf\Omega\mapsto m$, and $p:\mbf w\in\mbf\Omega\mapsto p^{c(\mbf w)}(\mbf w)$.
This way, we can define $T:\mbf\Omega\to\Theta$ by $T(\mbf w)=T^{c(\mbf w)}(p(\mbf w))$ and $\overline{\mbf Y}=(Y_j)_{j\geq1}$ with for any $j\geq1$, $Y_j:\mbf\Omega\to\sY$ by $Y_j(\mbf w)=Y_j^{c(\mbf w)}(p(\mbf w))$.\\
We can verify that $T$ is a measurable: if $B\in\sT$,  
    \begin{equation}
        T^{-1}(B) = \bigcup_{i\in I}\{\mbf w\in\mbf\Omega,\, c(\mbf w)=i,\,T^i(p(\mbf w))\in B  \} = \bigcup_{i\in I}c^{-1}(\{i\})\cap (T^i(p^i))^{-1}(B)
    \end{equation}
which is measurable. 
%\textcolor{red}{Probleme : que veut dire $T^i(p({\mbf w}))$ quand ${\mbf w}$ est de la forme $(i', ...)$ ? ce devrait etre $T^i(\omega^{i'})$ qui ne veut rien dire.}
The same arguments stand for the measurability of $Y_j$ for any $j$.




%\color{black}



We have,
for any $V\in\sT$:
    \begin{align}
          \mbf\Pi(T\in V) &= \overline{\varPi}\Big(\bigcup_{i\in I}\{i\}\times\Omega^1\times\dots\times\{T^i\in V\}\times\Omega^{i+1}\times\dots\Big)\\
            %&
          %I\times\prod_{i\in I} \{T^i\in V\}\Big) = 
          &=\sum_{i\in I}\PP^i(T^i\in V)\varPi(U_i) = \sum_{i\in I}\varPi(V|U_i)\varPi(U_i) = \varPi(V);\nonumber
    \end{align}
and if $\varPi(V)<\infty$ then for any $(A_{j_l})_{l=1}^k\in\sY^k$: %, with the notation $\{Y_{i_l}\in A_{i_l}\}=\{Y_{i_l}(p)\in A_{i_l}\}$ to see that event as en event in $\mbf\Omega$:
    \begin{multline}
        \mbf\Pi\left((Y_{j_l})_{l=1}^k\in(A_{j_l})_{l=1}^k|T\in V\right)\mbf\Pi(T\in V) \\ = 
        \mbf\Pi\Big(\bigcup_{i\in I}  \{i\}\times\Omega^1\times\dots\{(Y^i_{jl})_{l=1}^k\in(A_{jl})_{l=1}^k\}\cap \{T^i\in V \}\times\Omega^{i+1}\dots 
        \Big),
    \end{multline}
    so that
        \begin{align}\nonumber
        &\mbf\Pi\left((Y_{j_l})_{l=1}^k\in(A_{j_l})_{l=1}^k|T\in V\right) 
            = \sum_{i\in I}\PP^i\left( \{(Y_{j_l}^i)_{l=1}^k\in(A_{j_l})_{l=1}^k\}\cap \{T^i\in V\}\right)\frac{\varPi(U_i)}{\varPi(V)} \\
            &= \sum_{i\in I}\int_{V\cap U_i}\PP^{\otimes k}_{Y|\theta}\left((A_{j_l}\right)_{l=1}^k)\frac{d\varPi(\theta)}{\varPi(V)}  = \int_\Theta\PP^{\otimes k}_{Y|\theta}\left((A_{j_l})_{l=1}^k\right)d\varPi(\theta|V).
    \end{align}
%consequently to $\PP^i(A_j, T^i\in V)=\int_{V}\PP^\otimes(A_j)\pi(\theta|U_i)=\int_{U_i\cap V}\PP^\otimes(A_j)d\pi(\theta)/\pi(U_i)$


% \textcolor{red}{Revoir les calculs precedents et les suivants. On a (pour $Y_1$ ou pour $T$)
% \begin{align*}
%    \{ Y_1\in A_1 \} &= \big\{ {\mbf w} \in{\mbf \Omega} , Y_1({\mbf w})\in A \big\} \\
%    &= \bigcup_{i \in I} \big\{ (i, \omega^1,\omega^2,\ldots) \in{\mbf \Omega} , Y_1({\mbf w})\in A \big\} \\
%    &= \bigcup_{i \in I} \big\{ (i, \omega^1,\omega^2,\ldots) \in{\mbf \Omega} , Y_1^i( \omega^i)\in A \big\} \\
%     &= \bigcup_{i \in I} \Big( \{i\} \times \Omega^1 \times \Omega^2 \times \cdots \times \{ \omega^i \in \Omega^i, Y_1^i( \omega^i)\in A \}\times \Omega^{i+1}\times \cdots \Big)
% \end{align*}
% mais on n'a pas 
% $$
% \{ Y_1\in A_1 \} =  I \times \prod_{i\in I} \{ Y_1^i\in A\}
% $$
% ni
% $$
% \{T \in V\} = I \times \prod_{i\in I} \{T^i\in V\}
% $$
% }

We precise that the above expressions rely on the fact that $\mbf\Pi(I\times\prod_{i\in I}E_i)=\sum_{i\in I}\PP^i(E_i)\varPi(U_i)$, for any $E_i\in\sX^i$, for every ${i\in I}$. To state that equality, we write $I\times\prod_{i\in I}E_i=\bigcup_{n\geq1}\bigcap_{k<\mbf i}(I^n\times\prod_{i\in I}E_{i,k}') $ with $I^n=\{i\in I,\,i< n\}$ and $E_{i,k}'=E_i$ if $i\leq k$, $E_{i,k}'=\Omega^i$ otherwise. This way for any $n\geq1$:
$I^n\times\prod_{i\in I} E_{i,k}' \subset I^{n+1}\times\prod_{i\in I} E_{i,k}'$ for any $k<\mbf i$, and
$        \bigcap_{k<\mbf i} I^n\times\prod_{i\in I} E_{i,k}'\subset \bigcap_{k<\mbf i} I^{n+1}\times\prod_{i\in I} E_{i,k}'$.
Thus,
    \begin{equation}\label{eq:proofKolmLimPiCup}
        \mbf\Pi\Big(I\times\prod_{i\in I}E_i\Big) 
            = \lim_{n\rightarrow\infty}\mbf\Pi\Big(\bigcap_{k<\mbf i}(I^n\times\prod_{i\in I}E_{i,k}')\Big).
    \end{equation}


%    \begin{align}
%        &I^n\times\prod_{i\in I} E_{i,k}' \subset I^{n+1}\times\prod_{i\in I} E_{i,k}'
%        \quad\text{for any $k<\mbf i$,} \\
%        \text{and}\quad&
%        \bigcap_{k<\mbf i} I^n\times\prod_{i\in I} E_{i,k}'\subset \bigcap_{k<\mbf i} %I^{n+1}\times\prod_{i\in I} E_{i,k}'.
%    \end{align}
%Thus,
%    \begin{equation}\label{eq:proofKolmLimPiCup}
%        \mbf\varPi\Big(I\times\prod_{i\in I}E_i\Big) 
%            = \lim_{n\rightarrow\infty}\mbf\varPi\Big(\bigcap_{k<\mbf i}(I^n\times%\prod_{i\in I}E_{i,k}')\Big).
%    \end{equation}
For any $n\geq1$, $ %\bigcap_{k<\mbf i}
(I^n\times\prod_{i\in I}E_{i,k}')$ is a decreasing sequence, because $E_{i,k+1}'\subset E_{i,k}'$ for any $k<\mbf i-1$. 
Also, $\mbf\Pi(I^n\times\prod_{i\in I}E_{i,0}')=\sum_{i=0}^{n-1}\varPi(U_i)<\infty$.
Therefore, we can write:
    \begin{align}
        \mbf\Pi\Big(\bigcap_{k<\mbf i}(I^n\times\prod_{i\in I}E_{i,k}')\Big) 
            &= \lim_{k\rightarrow\infty} \mbf\Pi\Big(I^n\times\prod_{i<k,\mbf i}E_i\times\prod_{k\leq i<\mbf i}\Omega^i\Big) \\
            &= \lim_{k\rightarrow\infty} \Big[\sum_{i<k,n,\mbf i}\PP^i(E_i)\varPi(U_i)  + \sum_{k\leq i<n,\mbf i}\varPi(U_i)\Big]=\sum_{i<n,\mbf i}\PP^i(E_i)\varPi(U_i),\nonumber
    \end{align}
because the sums are all finite. Eventually, the limit in \cref{eq:proofKolmLimPiCup} is equal to $\sum_{i<\mbf i}\PP^i(E_i)\varPi(U_i)$ as expected, hence the result.






\section{Conclusive remarks}\label{sec:intro-ref:conclusion}


%This section should conclude the state-of-the-art and delimit the limitations of the current theory on different aspects. 

Reference priors are designed 
as a solution to the search for objective priors.
To do so, they are constructed on informatic-theoretic foundations, under a framework though to ensure they maximize the role of the data in the definition of the posterior distribution.


The theory is provided with a formal definition and
several works propose rigorous studies of reference priors, in different contexts.
That let them be elected for their lack of subjectivity in various practical studies (e.g. \cite{chen_properties_2008,gu_parallel_2016,dandrea_objective_2021})
and various statistical models, such as Gaussian process-based models \citep{paulo_default_2005,gu_parallel_2016}, generalized linear models \citep{natarajan_reference_2000} or general exponential family models \citep{clarke_reference_2010}.

Moreover, the framework that we have introduced allows a rigorous incorporation of improper priors in Bayesian modelings.
This provides a better interpretability of the reference priors when it leads to an improper prior distribution. 




%\citep{Paulo2005,Mure2019,Natarajan2000}. %Yet, they are criticized for their  low computational feasibility.


%\citep{Berger1996, Clarke1996, Berger2001, Olivera2007}. Its expression within different contexts has been derived and studied, for example, concerning Gaussian process based models \citep{Paulo2005, Gu2018, Mure2019, Mure2021}, Gaussian hierarchical models \citep{Keefe2019}, generalized linear models \citep{Natarajan2000} or general exponential family models \citep{Clarke2010}
%Reference priors are used in various statistical models, such as Gaussian process-based models Paulo(2005); Gu and Berger (2016), generalized linear models Natarajan and Kass (2000), and even BayesianNeural Networks Gao et al. (2022). The RPs are recognized for their objective nature in practicalstudies D’Andrea et al. (2021); Li and Gu (2021); Van Biesbroeck et al. (2024),
%the theory is well developed the priors are studied the come with tremendious properties and even solutions in some cases.*



%Within this landscape, reference prior theory stands out as a principled and rigorous approach to objective Bayesian inference. Developed to provide a formal mechanism for constructing priors that maximize the expected information from data, reference priors offer a coherent solution to the challenge of noninformativity. By relying on information-theoretic foundations and maintaining critical invariance properties, they occupy a unique position in the toolbox of the modern Bayesian analyst.


However, some limits remain. 
First, the theory is still built on some choices, such as the choice of the dissimilarity measure to quantify how different are the prior and the posterior.
Second, the incorporation of improper priors is appealing yet can represent an issue in cases where they issue improper posteriors. Actually, reference priors are known to lead to improper posteriors in some cases.
Last, the theoretical developments limit their studies to sets of priors that are the more general possible, and effects of its restriction are rarely focused.












